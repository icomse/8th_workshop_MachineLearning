{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqttFuOEnQB_"
   },
   "source": [
    "## Variational Autoencoders\n",
    "### Notebook content modified from code given in the \"Variational Autoencoder\" section of \"Deep Learning for Molecules and Materials\\\" textbook (https://dmol.pub/dl/VAE.html)\n",
    "- ### White, A. D. (2022). Deep learning for molecules and materials. Living journal of computational molecular science, 3(1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CIllsTgioGO7",
    "outputId": "10a1e2b4-b8a6-4681-8583-c418ee8d24e4"
   },
   "outputs": [],
   "source": [
    "#Install packages\n",
    "!pip install numpy matplotlib seaborn jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-PcAbOgitHl"
   },
   "source": [
    "## VAE for Discrete Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXGkscliitHl"
   },
   "source": [
    "Note: The features are classes; we *are not* trying to make a classifier that takes in features and outputs classes. VAEs are for unlabeled data.\n",
    "\n",
    "\n",
    "Our first example will be to generate new example classes from a distribution of possible classes. An application for this might be to sample conditions of an experiment.\n",
    "- **Q: Can you think of any other applications for using a VAE with discrete data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4-FG9nI-LbL"
   },
   "source": [
    "Task overview:\n",
    "- Features: $x$, represented as one-hot vectors corresponding to a class\n",
    "- Goal: Learn the distribution $P(x)$ so that we can sample new $x$'s\n",
    " - **Q: Along with being able to sample new points, what else can we do after learning the latent space?**\n",
    "   - Learning the latent space can also provide a way to embed your features into low dimensional continuous vectors, allowing you to do things like optimization because you've moved from discrete classes to continuous vectors\n",
    "   - This is an extra benefit, our loss and training goal are to create a new $P(x)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8F7LL9npB4Oz"
   },
   "source": [
    "Implementing the encoder & decoder:\n",
    "- **Q: What are the inputs & outputs for the encoder?**\n",
    " - The encoder $q_\\phi(z | x)$ should output a *probability distribution* for vectors of real numbers $z$ and take an input of a one-hot vector $x$\n",
    " - **Q: How can we have the encoder output a probability distribution? Is there a simpler way to do this?**\n",
    "   - We defined $P(z)$ to be normally distributed, let's assume that the form of $q_\\phi(z | x)$ should be normal. Then our neural network could output the parameters to a normal distribution (mean/variance) for $z$, rather than trying to output a probability at every possible $z$ value. It's up to you if you want to have $q_\\phi(z | x)$ output a D-dimensional Gaussian distribution with a covariance matrix or just output D independent normal distributions. Having $q_\\phi(z | x)$ output a normal distribution also allows us to analytically simplify the expectation/integral in the KL-divergence term.\n",
    "\n",
    "- **Q: What are the inputs & outputs for the decoder?**\n",
    "  - The decoder $p_\\theta(x | z)$ should output a probability distribution over classes given the input, a real vector $z$.\n",
    "   - We can use the same form we use for classification: softmax activation. Just remember that we're not trying to output a specific $x$, just a probability distribution of $x$'s.\n",
    "   - Softmax is similar to the sigmoid function, but is used when there are multiple classes\n",
    "\n",
    "- **Q: What parameters do we have control over/can change for the VAE?**\n",
    "  - The hyperparameters of the encoder and decoder and the size of $z$. It makes sense to have the encoder and decoder share as many hyperparameters as possible, since they're somewhat symmetric. Just remember that the encoder in our example is outputting a mean and variance, which means using regression, and the decoder is outputting a normalized probability vector, which means using softmax. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_baACtRbitHl"
   },
   "source": [
    "### The Data\n",
    "The data is 1024 points $\\vec{x}_i$ where each $\\vec{x}_i$ is a 32 dimensional one-hot vector indicating class. We won't define the classes -- the data is synthetic. Since a VAE is unsupervised learning, there are no labels. Let's start by examining the data. We'll sum the occurrences of each class to see what the distribution of classes looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OnfBVML7itHm"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "import seaborn as sns\n",
    "import jax.numpy as jnp\n",
    "from jax.example_libraries import optimizers\n",
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FoF43HditHn"
   },
   "outputs": [],
   "source": [
    "# Generate data to use for training (no labels, just a vector indicating the class a specific x_i belongs to)\n",
    "sampled_z = np.random.choice([0, 1], size=1024) # Generates a random array of 1s and 0s (size = 1024)\n",
    "\n",
    "# Generate array of shape (1024, 32), where each entry is a one-hot vector of length 32\n",
    "data = ((sampled_z + 1) % 2) * np.random.normal(\n",
    "    size=sampled_z.shape, loc=-1, scale=0.5\n",
    ") + sampled_z * np.random.normal(size=sampled_z.shape, loc=1, scale=0.25)\n",
    "\n",
    "nbins = 32\n",
    "_, bins = np.histogram(data, bins=nbins)\n",
    "\n",
    "class_data = np.apply_along_axis(lambda x: np.histogram(x, bins)[0], 1, data.reshape(-1, 1))\n",
    "\n",
    "nclasses = nbins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dv0PzlSPFoZ-",
    "outputId": "65b32d67-bb64-4764-b88b-2265ea1f3148"
   },
   "outputs": [],
   "source": [
    "# Print out a couple x_i values\n",
    "print(f'x_8: {class_data[8]}')\n",
    "print(f'x_24: {class_data[24]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "h_8pL93iitHn",
    "outputId": "758db86c-5965-4ad4-e473-08a19fb5696c"
   },
   "outputs": [],
   "source": [
    "# Visualize distribution of classes\n",
    "plt.bar(np.arange(nclasses), height=np.sum(class_data, axis=0))\n",
    "plt.xlabel(\"Class Index\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71l4b5GVitHn"
   },
   "source": [
    "### The Encoder & Decoder\n",
    "\n",
    "Our encoder will be a basic two hidden layer network.\n",
    "\n",
    "We will output a $D\\times2$ matrix, where the first column is means and the second is standard deviations for independent normal distributions that make up our guess for $q(z | x)$. Outputting a mean is simple, just use no activation. Outputting a standard deviation is unusual because they should be on $(0, \\infty)$. `jax.nn.softplus` can accomplish this.\n",
    "\n",
    "The decoder should output a vector of probabilities for $\\vec{x}$. This can be achieved by just adding a softmax to the output. The rest is nearly identical to the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZfd3TkZitHn"
   },
   "outputs": [],
   "source": [
    "# Helper function for generating a random vector of a specific size from the normal distribution\n",
    "def random_vec(size):\n",
    "    return np.random.normal(size=size, scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7X5-n9nKitHn",
    "outputId": "dd99c222-545c-4a5d-881f-433b78e44028"
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "latent_dim = 1\n",
    "hidden_dim = 16\n",
    "input_dim = nclasses # = 32; defined this above when generating the data\n",
    "\n",
    "\n",
    "def encoder(x, theta):\n",
    "    \"\"\"The encoder takes as input x and gives out probability of z,\n",
    "    expressed as normal distribution parameters. Assuming each z dim is independent,\n",
    "    output |z| x 2 matrix\"\"\"\n",
    "    w1, w2, w3, b1, b2, b3 = theta #get weights\n",
    "    hx = jax.nn.relu(w1 @ x + b1) #hidden layer 1 (dense layer)\n",
    "    hx = jax.nn.relu(w2 @ hx + b2) #hidden layer 2 (dense layer)\n",
    "    out = w3 @ hx + b3 #output - get mean & standard deviation from this\n",
    "    # slice out stddeviation and make it positive\n",
    "    reshaped = out.reshape((-1, 2))\n",
    "    # we slice with ':' to keep rank same\n",
    "    std = jax.nn.softplus(reshaped[:, 1:])\n",
    "    mu = reshaped[:, 0:1]\n",
    "    return jnp.concatenate((mu, std), axis=1)\n",
    "\n",
    "\n",
    "def init_theta(input_dim, hidden_units, latent_dim):\n",
    "    \"\"\"Create inital theta parameters\"\"\"\n",
    "    w1 = random_vec(size=(hidden_units, input_dim))\n",
    "    b1 = np.zeros(hidden_units)\n",
    "    w2 = random_vec(size=(hidden_units, hidden_units))\n",
    "    b2 = np.zeros(hidden_units)\n",
    "    # need two params per dim (mean, std)\n",
    "    w3 = random_vec(size=(latent_dim * 2, hidden_units))\n",
    "    b3 = np.zeros(latent_dim * 2)\n",
    "    return [w1, w2, w3, b1, b2, b3]\n",
    "\n",
    "\n",
    "# test encoder\n",
    "theta = init_theta(input_dim, hidden_dim, latent_dim)\n",
    "encoder_result = encoder(class_data[0], theta)\n",
    "print(f'Shape of encoder output: {np.shape(encoder_result)}')\n",
    "encoder_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85bC83hoitHo",
    "outputId": "a3a346d5-9c1d-4083-a95d-c367dedba4e6"
   },
   "outputs": [],
   "source": [
    "def decoder(z, phi):\n",
    "    \"\"\"decoder takes as input the latent variable z and gives out probability of x.\n",
    "    Decoder outputes a real number, then we use softmax activation to get probability across\n",
    "    possible values of x.\n",
    "    \"\"\"\n",
    "    w1, w2, w3, b1, b2, b3 = phi\n",
    "    hz = jax.nn.relu(w1 @ z + b1) #hidden layer\n",
    "    hz = jax.nn.relu(w2 @ hz + b2) #hidden layer\n",
    "    out = jax.nn.softmax(w3 @ hz + b3) #output\n",
    "    return out\n",
    "\n",
    "\n",
    "def init_phi(input_dim, hidden_units, latent_dim):\n",
    "    \"\"\"Create inital phi parameters\"\"\"\n",
    "    w1 = random_vec(size=(hidden_units, latent_dim))\n",
    "    b1 = np.zeros(hidden_units)\n",
    "    w2 = random_vec(size=(hidden_units, hidden_units))\n",
    "    b2 = np.zeros(hidden_units)\n",
    "    w3 = random_vec(size=(input_dim, hidden_units))\n",
    "    b3 = np.zeros(input_dim)\n",
    "    return [w1, w2, w3, b1, b2, b3]\n",
    "\n",
    "\n",
    "# test decoder\n",
    "phi = init_phi(input_dim, hidden_dim, latent_dim)\n",
    "decoder_result = decoder(np.array([1.2] * latent_dim), phi)\n",
    "print(f'Shape of decoder output: {np.shape(decoder_result)}')\n",
    "decoder_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-blfvbZitHo"
   },
   "source": [
    "### Training\n",
    "\n",
    "We use ELBO equation for training:\n",
    "\n",
    "$$\n",
    "l = -\\textrm{E}_{z \\sim q_\\phi(z | x_i)}\\left[\\log p_{\\theta}(x_i | z)\\right] + \\textrm{KL}\\left[(q_\\phi(z | x))|| P(z)\\right]\n",
    "$$\n",
    "\n",
    "where $P(z)$ is the standard normal distribution and we approximate expectations using a single sample from the encoder. We need to expand the KL-divergence term to implement this. Both $P(z)$ and $q_\\theta(z | x)$ are normal. You can look-up the KL-divergence between two normal distributions:\n",
    "\n",
    "\\begin{equation}\n",
    "KL(q, p) = \\log \\frac{\\sigma_p}{\\sigma_q} + \\frac{\\sigma_q^2 + (\\mu_q - \\mu_p)^2}{2 \\sigma_p^2} - \\frac{1}{2}\n",
    "\\end{equation}\n",
    "\n",
    "we can simplify because $P(z)$ is standard normal ($\\sigma = 1, \\mu = 0$)\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{KL}\\left[(q_\\theta(z | x_i))|| P(z)\\right] = -\\log \\sigma_i + \\frac{\\sigma_i^2}{2} + \\frac{\\mu_i^2}{2} - \\frac{1}{2}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mu_i, \\sigma_i$ are the output from $q_\\phi(z | x_i)$.\n",
    "\n",
    "For a latent space dimension of 3, $x_i$ has the shape (1,n_classes), $\\sigma_i$ has the shape (1,3), and $\\mu_i$ has the shape (1,3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3B2kFpWLitHp",
    "outputId": "a529706d-bbc5-427d-bf08-debde1dbb117"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss(x, theta, phi, rng_key):\n",
    "    \"\"\"VAE Loss\"\"\"\n",
    "    # reconstruction loss\n",
    "    sampled_z_params = encoder(x, theta)\n",
    "    # reparameterization trick\n",
    "    # we use standard normal sample and multiply by parameters\n",
    "    # to ensure derivatives correctly propogate to encoder\n",
    "    sampled_z = (\n",
    "        jax.random.normal(rng_key, shape=(latent_dim,)) * sampled_z_params[:, 1]\n",
    "        + sampled_z_params[:, 0]\n",
    "    )\n",
    "    # log of prob\n",
    "    rloss = -jnp.log(decoder(sampled_z, phi) @ x.T + 1e-8)\n",
    "    # Array of KL loss for x_i (dimension = latent space dimension)\n",
    "    klloss = (\n",
    "        -0.5\n",
    "        - jnp.log(sampled_z_params[:, 1])\n",
    "        + 0.5 * sampled_z_params[:, 0] ** 2\n",
    "        + 0.5 * sampled_z_params[:, 1] ** 2\n",
    "    )\n",
    "    # combined\n",
    "    return jnp.array([rloss, jnp.mean(klloss)])\n",
    "\n",
    "\n",
    "# test loss function\n",
    "loss(class_data[0], theta, phi, jax.random.PRNGKey(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MtLw4PoitHp"
   },
   "source": [
    "Our loss works! Now we need to make it batched so we can train in batches. Luckily this is easy with `vmap<jax.vmap>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hOAzOxWSitHp",
    "outputId": "abf5ecd4-4830-4412-95b6-908dd6ad7908"
   },
   "outputs": [],
   "source": [
    "batched_loss = jax.vmap(loss, in_axes=(0, None, None, None), out_axes=0)\n",
    "batched_decoder = jax.vmap(decoder, in_axes=(0, None), out_axes=0)\n",
    "batched_encoder = jax.vmap(encoder, in_axes=(0, None), out_axes=0)\n",
    "\n",
    "# test batched loss\n",
    "batched_loss(class_data[:4], theta, phi, jax.random.PRNGKey(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adiDYt3MitHp"
   },
   "source": [
    "We'll make our gradient take the average over the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ztQqWANitHp"
   },
   "outputs": [],
   "source": [
    "grad = jax.grad(\n",
    "    lambda x, theta, phi, rng_key: jnp.mean(batched_loss(x, theta, phi, rng_key)),\n",
    "    (1, 2),\n",
    ")\n",
    "fast_grad = jax.jit(grad)\n",
    "fast_loss = jax.jit(batched_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mmr7XmtlitHq"
   },
   "source": [
    "Alright, great! An important detail we've skipped so far is that when using `jax` to generate random numbers, we must step our random number generator forward. You can do that using `jax.random.split`. Otherwise, you'll get the same random numbers at each draw.\n",
    "\n",
    "We're going to use a `jax` optimizer here. This is to simplify parameter updates. We have a lot of parameters and they are nested, which will be complex for treating with python for loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIKRFAz5itHq"
   },
   "outputs": [],
   "source": [
    "#Train for 16 epochs\n",
    "batch_size = 32\n",
    "epochs = 16\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "opt_init, opt_update, get_params = optimizers.adam(step_size=1e-1)\n",
    "theta0 = init_theta(input_dim, hidden_dim, latent_dim)\n",
    "phi0 = init_phi(input_dim, hidden_dim, latent_dim)\n",
    "opt_state = opt_init((theta0, phi0))\n",
    "losses = []\n",
    "for e in range(epochs):\n",
    "    for bi, i in enumerate(range(0, len(data), batch_size)):\n",
    "        # make a batch into shape B x 1\n",
    "        batch = class_data[i : (i + batch_size)]\n",
    "        # update random number key\n",
    "        key, subkey = jax.random.split(key)\n",
    "        # get current parameter values from optimizer\n",
    "        theta, phi = get_params(opt_state)\n",
    "        last_state = opt_state\n",
    "        # compute gradient and update\n",
    "        grad = fast_grad(batch, theta, phi, key)\n",
    "        opt_state = opt_update(bi, grad, opt_state)\n",
    "        lvalue = jnp.mean(fast_loss(batch, theta, phi, subkey), axis=0)\n",
    "        losses.append(lvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "Jae43claitHq",
    "outputId": "3c1e91b3-aa24-4fd1-d3fa-d02f48bcac6a"
   },
   "outputs": [],
   "source": [
    "#Plot loss vs epoch results\n",
    "plt.plot([l[0] for l in losses], label=\"Reconstruction\")\n",
    "plt.plot([l[1] for l in losses], label=\"KL\")\n",
    "plt.plot([l[1] + l[0] for l in losses], label=\"ELBO\")\n",
    "plt.legend()\n",
    "plt.ylim(-5, 5)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1svlzWIitHr"
   },
   "source": [
    "### Evaluating the VAE\n",
    "\n",
    "Remember our goal with the VAE is to reproduce $P(x)$. We can sample from our VAE using the chosen $P(z)$ and our decoder. Let's compare that distribution with our training distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "zkczIE_2itHr",
    "outputId": "2e44ba17-ef97-43ea-b4e9-a20e6d7e7583"
   },
   "outputs": [],
   "source": [
    "#Plot distribution of training data & values sampled from VAE\n",
    "\n",
    "zs = np.random.normal(size=(1024, 1))\n",
    "sampled_x = batched_decoder(zs, phi)\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(8, 4))\n",
    "axs[0].set_title(\"Training Data\")\n",
    "axs[0].bar(np.arange(nbins), height=np.sum(class_data, axis=0))\n",
    "axs[0].set_xlabel(\"Class Index\")\n",
    "axs[0].set_ylabel(\"Frequency\")\n",
    "axs[1].set_title(\"VAE Samples\")\n",
    "axs[1].bar(np.arange(nbins), height=np.sum(sampled_x, axis=0))\n",
    "axs[1].set_xlabel(\"Class Index\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UQB5vKxitHr"
   },
   "source": [
    "It appears we have succeeded! There were two more goals of the VAE model: making the encoder give output similar to $P(z)$ and be able to reconstruct. These goals are often opposed and they represent the two terms in the loss: reconstruction and KL-divergence. Let's examine the KL-divergence term, which causes the encoder to give output similar to a standard normal. We'll sample from our training data in histogram look at the resulting average mean and std dev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uaacii7citHr",
    "outputId": "c45e9098-de73-45f7-a0f7-87e49664eccc"
   },
   "outputs": [],
   "source": [
    "d = batched_encoder(class_data, theta)\n",
    "print(\"Average mu = \", np.mean(d[..., 0]), \"Average std dev = \", np.mean(d[..., 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFSuLpL5itHr"
   },
   "source": [
    "Wow! Very close to a standard normal. So our model satisfied the match between the decoder and the $P(z)$. The last thing to check is reconstruction. These are distributions, so I'll only look at the maximum $z$ value to do the reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "HtoCgByvitHr",
    "outputId": "d7bd1fd8-1279-488b-aa8e-70c1ef1e19f4"
   },
   "outputs": [],
   "source": [
    "plt.plot(decoder(encoder(class_data[2], theta)[0:1, 0], phi), label=\"P(x)\")\n",
    "plt.axvline(np.argmax(class_data[2]), color=\"C1\", label=\"x\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0FwOp6xitHr"
   },
   "source": [
    "The reconstruction is not great, it puts a lot of probability mass on other points. In fact, the reconstruction seems to not use the encoder's information at all -- it looks like $P(x)$. The reason for this is that our KL-divergence term dominates. It has a very good fit.\n",
    "\n",
    "## Re-balancing VAE Reconstruction and KL-Divergence\n",
    "\n",
    "Often we desire more reconstruction at the cost of making the latent space less normal. This can be done by adding a term that adjusts the balance between the reconstruction loss and the KL-divergence. You would choose to do this if you want to use the latent space for something and are not just interested in creating a model $\\hat{P}(x)$. Here is the modified ELBO equation for training:\n",
    "\n",
    "$$\n",
    "l = -\\textrm{E}_{z \\sim q_\\phi(z | x_i)}\\left[\\log p_{\\theta}(x_i | z)\\right] + \\beta\\cdot\\textrm{KL}\\left[(q_\\phi(z | x))|| P(z)\\right]\n",
    "$$\n",
    "\n",
    "where $\\beta > 1$ emphasizes the encoder distribution matching chosen latent distribution (standard normal) and $\\beta < 1$ emphasizes reconstruction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onzuNJ5jitHr"
   },
   "outputs": [],
   "source": [
    "def modified_loss(x, theta, phi, rng_key, beta):\n",
    "    \"\"\"This loss allows you to vary which term is more important\n",
    "    with beta. Beta = 0 - all reconstruction, beta = 1 - ELBO\"\"\"\n",
    "    bl = batched_loss(x, theta, phi, rng_key)\n",
    "    l = bl @ jnp.array([1.0, beta])\n",
    "    return jnp.mean(l)\n",
    "\n",
    "\n",
    "new_grad = jax.grad(modified_loss, (1, 2))\n",
    "fast_grad = jax.jit(new_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpdBTwdIitHs"
   },
   "outputs": [],
   "source": [
    "#Train for 32 epochs\n",
    "#note we used a lower step size for this loss and more epochs\n",
    "opt_init, opt_update, get_params = optimizers.adam(step_size=5e-2)\n",
    "epochs = 32\n",
    "theta0 = init_theta(input_dim, hidden_dim, latent_dim)\n",
    "phi0 = init_phi(input_dim, hidden_dim, latent_dim)\n",
    "opt_state = opt_init((theta0, phi0))\n",
    "beta = 0.2\n",
    "losses = []\n",
    "for e in range(epochs):\n",
    "    for bi, i in enumerate(range(0, len(data), batch_size)):\n",
    "        # make a batch into shape B x 1\n",
    "        batch = class_data[i : (i + batch_size)]\n",
    "        # udpate random number key\n",
    "        key, subkey = jax.random.split(key)\n",
    "        # get current parameter values from optimizer\n",
    "        theta, phi = get_params(opt_state)\n",
    "        last_state = opt_state\n",
    "        # compute gradient and update\n",
    "        grad = fast_grad(batch, theta, phi, key, beta)\n",
    "        opt_state = opt_update(bi, grad, opt_state)\n",
    "        lvalue = jnp.mean(fast_loss(batch, theta, phi, subkey), axis=0)\n",
    "        losses.append(lvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "-2s71S6NitHs",
    "outputId": "5bc98846-8acd-44af-dde3-fa8916d8f20e"
   },
   "outputs": [],
   "source": [
    "#Plot Loss vs Epoch results\n",
    "plt.plot([l[0] for l in losses], label=\"Reconstruction\")\n",
    "plt.plot([l[1] for l in losses], label=\"KL\")\n",
    "plt.plot([l[1] + l[0] for l in losses], label=\"ELBO\")\n",
    "plt.legend()\n",
    "plt.ylim(-5, 5)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY33f-v5itHs"
   },
   "source": [
    "You can see the error is higher, but let's see how it did at our three metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "J4Stmj7AitHs",
    "outputId": "eea1f5e1-ce99-4b0f-dffb-1cd9c0078618"
   },
   "outputs": [],
   "source": [
    "#Plot distribution of training data & values sampled from VAE\n",
    "zs = np.random.normal(size=(1024, 1))\n",
    "sampled_x = batched_decoder(zs, phi)\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(8, 4))\n",
    "axs[0].set_title(\"Training Data\")\n",
    "axs[0].bar(np.arange(nbins), height=np.sum(class_data, axis=0))\n",
    "axs[0].set_xlabel(\"Class Index\")\n",
    "axs[0].set_ylabel(\"Frequency\")\n",
    "axs[1].set_title(\"VAE Samples\")\n",
    "axs[1].bar(np.arange(nbins), height=np.sum(sampled_x, axis=0))\n",
    "axs[1].set_xlabel(\"Class Index\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mA65pDmCitHs"
   },
   "source": [
    "A little bit worse on $P(x)$, but overall not bad. What about our goal, the reconstruction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "szREHaKXitHs",
    "outputId": "8be705fa-c46a-43df-d277-c825b5eadb1c"
   },
   "outputs": [],
   "source": [
    "plt.plot(decoder(encoder(class_data[4], theta)[0:1, 0], phi), label=\"P(x)\")\n",
    "plt.axvline(np.argmax(class_data[4]), color=\"C1\", label=\"x\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoq1w-GoitHs"
   },
   "source": [
    "What about our encoder's agreement with a standard normal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m7AIW3p9itHs",
    "outputId": "0b6cb62d-71a2-487a-8a12-38fb028819bc"
   },
   "outputs": [],
   "source": [
    "d = batched_encoder(class_data, theta)\n",
    "print(\"Average mu = \", np.mean(d[..., 0]), \"Average std dev = \", np.mean(d[..., 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sp1DBVP0itHt"
   },
   "source": [
    "The standard deviation is much smaller! So we squeezed our latent space a little at the cost of better reconstruction.\n",
    "\n",
    "### Disentangling $\\beta$-VAE\n",
    "\n",
    "You can adjust $\\beta$ the opposite direction, to value matching the prior Gaussian distribution more strongly. This can better condition the encoder so that each of the latent dimensions are truly independent. This can be important if you want to disentangle your input features to arrive at an orthogonal projection. This of course comes at the loss of reconstruction accuracy, but can be more important if you're interested in the latent space rather than generating new samples (Mathieu et al. 2019).\n",
    "\n",
    "References: <br>\n",
    "Mathieu, E., Rainforth, T., Siddharth, N., & Teh, Y. W. (2019, May). Disentangling disentanglement in variational autoencoders. In International conference on machine learning (pp. 4402-4412). PMLR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdAU8X7oitHt"
   },
   "source": [
    "## Bead-Spring Polymer VAE\n",
    "\n",
    "This polymer has each bead (atom) joined by a harmonic bond, a harmonic angle between each three, and a Lennard-Jones interaction potential. Knowing these items will not be necessary for the example. Each of our data points below will be the x and y coordinates of 12 beads. We'll construct a VAE that can compress the trajectory to some latent space and generate new conformations.\n",
    "\n",
    "Since we're now work with continuous features $x$. We need to make a few key changes. The encoder will remain the same, but the decoder now must output a $p_\\theta(x | z)$ that gives a probability to all possible $x$ values. Above, we only had a finite number of classes but now any $x$ is possible. As we did for the encoder, we'll assume that $p_\\theta(x | z)$ should be normal and we'll output the parameters of the normal distribution from our network. This requires an update to the reconstruction loss to be a log of a normal, but otherwise things will be identical.\n",
    "\n",
    "Don't worry about this too much but a small detail is that the log-likelihood for a normal distribution with a single observation cannot have unknown standard deviation. Our new normal distribution parameters for the decoder will have a single observation for a single $x$ in training. If you make the standard deviation trainable, it will just pick infinity as the standard deviation since that will for sure capture the point and you only have one point. Thus, I'll make the decoder standard deviation be a hyperparameter. We don't see this issue with the encoder, which also outputs a normal distribution, because we training the encoder with the KL-divergence term and not likelihood of observations (reconstruction loss).\n",
    "\n",
    "To begin, we'll need to align points from a trajectory in order to ensure the data is translationally and rotationally invariant. This will then serve as our training data. The space of our problem will be 12 2D vectors. Our system need not be permutation invariant, so we can flatten these vectors into a 24 dimensional input. The code belows loads and aligns the trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-oYvIcBLWnII"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax.example_libraries import optimizers\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MdPp9-IsitHu"
   },
   "outputs": [],
   "source": [
    "###---------Transformation Functions----###\n",
    "# for rotational/translational invariance\n",
    "\n",
    "def center_com(paths):\n",
    "    \"\"\"Align paths to COM at each frame\"\"\"\n",
    "    # center of mass\n",
    "    coms = np.mean(paths, axis=-2, keepdims=True)\n",
    "    return paths - coms\n",
    "\n",
    "def make_2drot(angle):\n",
    "    \"\"\"Defines a rotation matrix\"\"\"\n",
    "    mats = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])\n",
    "    # swap so batch axis is first\n",
    "    return np.swapaxes(mats, 0, -1)\n",
    "\n",
    "def find_principle_axis(points):\n",
    "    \"\"\"Compute single principle axis for points\"\"\"\n",
    "    inertia = points.T @ points\n",
    "    evals, evecs = np.linalg.eigh(inertia)\n",
    "    # get biggest eigenvalue\n",
    "    order = np.argsort(evals)\n",
    "    return evecs[:, order[-1]]\n",
    "\n",
    "def align_principle(paths, axis_finder=find_principle_axis):\n",
    "    \"\"\"Rotates the data at each frame\n",
    "    so that the principle axis remains constant\"\"\"\n",
    "    vecs = [axis_finder(p) for p in paths]\n",
    "    vecs = np.array(vecs)\n",
    "    # find angle to rotate so these are pointed towards pos x\n",
    "    cur_angle = np.arctan2(vecs[:, 1], vecs[:, 0])\n",
    "    rot_angle = -cur_angle\n",
    "    rot_mat = make_2drot(rot_angle)\n",
    "    return paths @ rot_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "Mj8ucNLjitHu",
    "outputId": "a68386fc-9f61-48d1-b057-594e625a0c2e"
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://github.com/whitead/dmol-book/raw/main/data/long_paths.npz\",\n",
    "    \"long_paths.npz\",\n",
    ")\n",
    "paths = np.load(\"long_paths.npz\")[\"arr\"]\n",
    "# transform to be rot/trans invariant\n",
    "data = align_principle(center_com(paths))\n",
    "# visualize all the data\n",
    "cmap = plt.get_cmap(\"cool\")\n",
    "for i in range(0, data.shape[0], 16):\n",
    "    plt.plot(data[i, :, 0], data[i, :, 1], \"-\", alpha=0.1, color=\"C2\")\n",
    "plt.title(\"All Frames\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "d4XcFnKPasjz",
    "outputId": "667e8304-d5a2-4f5f-819f-396fbbe3bb7c"
   },
   "outputs": [],
   "source": [
    "# visualize a single data point\n",
    "plt.plot(data[3, :, 0], data[3, :, 1], \"-\", color=\"C2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjUP5btL6OJJ"
   },
   "source": [
    "Before training, let’s examine some of the marginals of the data to visualize the data. Marginals mean we’ve transformed (by integration) our probability distribution to be a function of only 1-2 variables so that we can plot nicely. We’ll look at the pairwise distance between beads (beads are indexed 0-11)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "sWEfq1946Rug",
    "outputId": "014db0a6-3313-41f1-b030-88ec6cc949a4"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=4, squeeze=True, figsize=(16, 4))\n",
    "for i, j in enumerate(range(1, 9, 2)):\n",
    "    axs[i].set_title(f\"Dist between 0-{j}\")\n",
    "    sns.histplot(np.linalg.norm(data[:, 0] - data[:, j], axis=1), ax=axs[i], kde=True, stat=\"density\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2C1zDip6xfa"
   },
   "source": [
    "These look a little like the chi distribution with two degrees of freedom. Notice that the support (x-axis) changes between them though. We’ll keep an eye on these when we evaluate the efficacy of our VAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctkaoHofitHu"
   },
   "source": [
    "### VAE Model\n",
    "\n",
    "Now we'll build the VAE similar to before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmjQRZp6itHv"
   },
   "outputs": [],
   "source": [
    "input_dim = 12 * 2 # 12 points each with x and y coordinates\n",
    "hidden_units = 256 # dimension of hidden layers in encoder/decoder\n",
    "num_layers = 4 # number of hidden layers\n",
    "latent_dim = 2 # dimension of z\n",
    "\n",
    "# randomly initialize weights for the decoder\n",
    "def init_theta(input_dim, hidden_units, latent_dim, num_layers, key, scale=0.1):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    # theta[0] takes a vector from latent layer to a hidden layer\n",
    "    w1 = jax.random.normal(key=subkey, shape=(hidden_units, latent_dim)) * scale\n",
    "    b1 = jnp.zeros(hidden_units)\n",
    "    theta = [(w1, b1)]\n",
    "    # theta[i] takes a vector from hidden layer to hidden layer\n",
    "    for i in range(1, num_layers - 1):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        w = jax.random.normal(key=subkey, shape=(hidden_units, hidden_units)) * scale\n",
    "        b = jnp.zeros(hidden_units)\n",
    "        theta.append((w, b))\n",
    "    key, subkey = jax.random.split(key)\n",
    "    # theta[-1] takes a vector from hidden layer to output layer\n",
    "    w = jax.random.normal(key=subkey, shape=(input_dim, hidden_units)) * scale\n",
    "    b = jnp.zeros(input_dim)\n",
    "    theta.append((w, b))\n",
    "    return theta, key\n",
    "\n",
    "def decoder(z, theta):\n",
    "    num_layers = len(theta)\n",
    "    for i in range(num_layers - 1):\n",
    "        w, b = theta[i]\n",
    "        # dense layer with relu activation\n",
    "        z = jax.nn.relu(w @ z + b)\n",
    "    w, b = theta[-1]\n",
    "    # dense layer to get output\n",
    "    x = w @ z + b\n",
    "    # returning x which is the mean of the distribution\n",
    "    return x\n",
    "\n",
    "# randomly initialize weights for the encoder\n",
    "def init_phi(input_dim, hidden_units, latent_dim, num_layers, key, scale=0.1):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    # phi[0] takes a vector from input layer to a hidden layer\n",
    "    w1 = jax.random.normal(key=subkey, shape=(hidden_units, input_dim)) * scale\n",
    "    b1 = jnp.zeros(hidden_units)\n",
    "    phi = [(w1, b1)]\n",
    "    # phi[i] takes a vector from hidden layer to hidden layer\n",
    "    for i in range(1, num_layers - 1):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        w = jax.random.normal(key=subkey, shape=(hidden_units, hidden_units)) * scale\n",
    "        b = jnp.zeros(hidden_units)\n",
    "        phi.append((w, b))\n",
    "    key, subkey = jax.random.split(key)\n",
    "    # phi[-1] takes a vector from hidden layer to latent layer\n",
    "    w = jax.random.normal(key=subkey, shape=(latent_dim * 2, hidden_units)) * scale\n",
    "    b = jnp.zeros(latent_dim * 2)\n",
    "    phi.append((w, b))\n",
    "    return phi, key\n",
    "\n",
    "# returns the mean and std deviation for the distribution in the latent space\n",
    "def encoder(x, phi):\n",
    "    num_layers = len(phi)\n",
    "    for i in range(num_layers - 1):\n",
    "        w, b = phi[i]\n",
    "        # dense layer with relu activation\n",
    "        x = jax.nn.relu(w @ x + b)\n",
    "    w, b = phi[-1]\n",
    "    # dense layer to get output\n",
    "    hz = w @ x + b\n",
    "    hz = hz.reshape(-1, 2)\n",
    "    mu = hz[:, 0:1]\n",
    "    # softplus ensures standard deviation is in the range [0, infty)\n",
    "    std = jax.nn.softplus(hz[:, 1:2])\n",
    "    return jnp.concatenate((mu, std), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRXcgsasitHv"
   },
   "source": [
    "### Loss\n",
    "\n",
    "The loss function is similar to above, but I will not bother sampling from the decoded distribution, and instead just take the value outputted from the decoder. You can see the only change is that we drop the output Gaussian standard deviation from the loss, which remember was not trainable anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8l2qlYMitHv"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss(x, theta, phi, rng_key):\n",
    "    \"\"\"VAE Loss\"\"\"\n",
    "    # reconstruction loss\n",
    "    sampled_z_params = encoder(x, phi)\n",
    "    # reparameterization trick\n",
    "    # we use standard normal sample and multiply by parameters\n",
    "    # to ensure derivatives correctly propogate to encoder\n",
    "    sampled_z = (\n",
    "        jax.random.normal(rng_key, shape=(latent_dim,)) * sampled_z_params[:, 1]\n",
    "        + sampled_z_params[:, 0]\n",
    "    )\n",
    "    # MSE to compute reconstruction loss\n",
    "    xp = decoder(sampled_z, theta)\n",
    "    rloss = jnp.sum((xp - x) ** 2)\n",
    "\n",
    "    # KL divergence loss\n",
    "    klloss = (\n",
    "        -0.5\n",
    "        - jnp.log(sampled_z_params[:, 1] + 1e-8)\n",
    "        + 0.5 * sampled_z_params[:, 0] ** 2\n",
    "        + 0.5 * sampled_z_params[:, 1] ** 2\n",
    "    )\n",
    "    # combined\n",
    "    return jnp.array([rloss, jnp.mean(klloss)])\n",
    "\n",
    "\n",
    "# update compiled functions\n",
    "# vmap allows a function to compute on a vector\n",
    "# by performing the operation entrywise\n",
    "batched_loss = jax.vmap(loss, in_axes=(0, None, None, None), out_axes=0)\n",
    "batched_decoder = jax.vmap(decoder, in_axes=(0, None), out_axes=0)\n",
    "batched_encoder = jax.vmap(encoder, in_axes=(0, None), out_axes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmnCcXl-64Cd"
   },
   "outputs": [],
   "source": [
    "# incoorporates rebalancing into loss\n",
    "def modified_loss(x, theta, phi, rng_key, beta):\n",
    "    \"\"\"This loss allows you to vary which term is more important\n",
    "    with beta. Beta = 0 - all reconstruction, beta = 1 - ELBO\"\"\"\n",
    "    bl = batched_loss(x, theta, phi, rng_key)\n",
    "    l = bl @ jnp.array([1.0, beta])\n",
    "    return jnp.mean(l)\n",
    "\n",
    "# use modified_loss to compute gradients\n",
    "grad = jax.grad(modified_loss, (1, 2))\n",
    "fast_grad = jax.jit(grad)\n",
    "fast_loss = jax.jit(batched_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GSOp7OuitHv"
   },
   "source": [
    "### Training\n",
    "\n",
    "Finally comes the training. We'll flatten our input data and shuffle to prevent each batch from having similar conformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwnyW2YmitHv"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 250\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "flat_data = data.reshape(-1, input_dim)\n",
    "# scramble it\n",
    "flat_data = jax.random.permutation(key, flat_data, independent = True)\n",
    "# optimizers from jax used to update parameters with stochastic gradient descent\n",
    "# step size is the same as learning rate\n",
    "opt_init, opt_update, get_params = optimizers.adam(step_size=1e-2)\n",
    "# initialize theta and phi randomly\n",
    "theta0, key = init_theta(input_dim, hidden_units, latent_dim, num_layers, key)\n",
    "phi0, key = init_phi(input_dim, hidden_units, latent_dim, num_layers, key)\n",
    "opt_state = opt_init((theta0, phi0))\n",
    "losses = []\n",
    "# KL/Reconstruction balance\n",
    "# beta close to 0 favors reconstruction (see the loss plot below)\n",
    "beta = 0.01\n",
    "\n",
    "for e in range(epochs):\n",
    "    # bi = batch number, i = index in the data\n",
    "    for bi, i in enumerate(range(0, len(flat_data), batch_size)):\n",
    "        # make a batch into shape B x 1\n",
    "        batch = flat_data[i : (i + batch_size)].reshape(-1, input_dim)\n",
    "        # update random number key\n",
    "        key, subkey = jax.random.split(key)\n",
    "        # get current parameter values from optimizer\n",
    "        theta, phi = get_params(opt_state)\n",
    "        last_state = opt_state\n",
    "        # compute gradient and update\n",
    "        grad = fast_grad(batch, theta, phi, key, beta)\n",
    "        opt_state = opt_update(bi, grad, opt_state)\n",
    "    # use large batch for tracking progress\n",
    "    lvalue = jnp.mean(fast_loss(flat_data[:100], theta, phi, subkey), axis=0)\n",
    "    losses.append(lvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "ULckDw8uitHv",
    "outputId": "bdaa6de5-69a3-4c78-d6b9-2263a4cbf51f"
   },
   "outputs": [],
   "source": [
    "plt.plot([l[0] for l in losses], label=\"Reconstruction\")\n",
    "plt.plot([l[1] for l in losses], label=\"KL\")\n",
    "plt.plot([l[1] + l[0] for l in losses], label=\"ELBO\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 20)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHwZm2mQitHv"
   },
   "source": [
    "This model is undertrained. A latent space of 2, which we chose for plotting convenience, is also probably a little too compressed. Let's sample a conformation and see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "biuwUVqWitHv",
    "outputId": "ca2962bb-5fa0-4eda-997b-6b0710ff388d"
   },
   "outputs": [],
   "source": [
    "sampled_data = decoder(jax.random.normal(key, shape=[latent_dim]), theta).reshape(-1, 2)\n",
    "plt.plot(sampled_data[:, 0], sampled_data[:, 1], \"-o\", alpha=1)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jLxfMus7HyI"
   },
   "source": [
    "### Generate New Samples\n",
    "\n",
    "Let's see how our samples look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "jEaH3HGh7N93",
    "outputId": "6aa72966-9573-4209-bbb4-c4c371603e91"
   },
   "outputs": [],
   "source": [
    "# visualize all the sampled data\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(12, 4))\n",
    "sampled_data = batched_decoder(\n",
    "    np.random.normal(size=(data.shape[0], latent_dim)), theta\n",
    ").reshape(data.shape[0], -1, 2)\n",
    "# visualize the training data\n",
    "for i in range(0, data.shape[0]):\n",
    "    axs[0].plot(data[i, :, 0], data[i, :, 1], \"-\", alpha=0.1, color=\"C2\")\n",
    "    axs[1].plot(\n",
    "        sampled_data[i, :, 0], sampled_data[i, :, 1], \"-\", alpha=0.1, color=\"C2\"\n",
    "    )\n",
    "axs[0].set_title(\"Training\")\n",
    "axs[1].set_title(\"Generated\")\n",
    "for i in range(2):\n",
    "    axs[i].set_xticks([])\n",
    "    axs[i].set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjU2FYqe7m6w"
   },
   "source": [
    "The samples are not perfect, but we're close. Let's examine the marginals to see how the distribution for the distances between beads for the generated data differs from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "LYKhU8Ns7j-i",
    "outputId": "86381d2d-0e75-4f97-d683-69b3808338a4"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=4, squeeze=True, figsize=(16, 4))\n",
    "for i, j in enumerate(range(1, 9, 2)):\n",
    "    axs[i].set_title(f\"Dist between 0-{j}\")\n",
    "    sns.histplot(np.linalg.norm(data[:, 0] - data[:, j], axis=1), ax=axs[i], kde=True, stat=\"density\", label=\"training distribution\")\n",
    "    sns.kdeplot(\n",
    "        np.linalg.norm(sampled_data[:, 0] - sampled_data[:, j], axis=1),\n",
    "        ax=axs[i], color=\"red\", label=\"sampled distribution\"\n",
    "    )\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YBhQ-Vm7sDn"
   },
   "source": [
    "You can see that there are some issues here as well. Remember that our  latent space is quite small: 2D. So we should not be that surprised that we're losing information from our 24D input space. I encourage you to play with some of the hyperparameters such as latent space dimension and beta to see if you can get better marginals."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
