{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01HCtQrK-6aT"
   },
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "6lbvPodqhzZD",
    "outputId": "8c297036-4052-40a6-e7a7-3d0945574477"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/icomse/8th_workshop_MachineLearning.git\n",
    "import os\n",
    "os.chdir('8th_workshop_MachineLearning/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "Tzjy2CdEiuNK",
    "outputId": "d072ef52-a065-45c9-f228-ab226d60e68d"
   },
   "outputs": [],
   "source": [
    "# Now we use pandas library to create a dataframe.\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_featurized=pd.read_csv('../data/featurized_mixture.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xr5iX_YXp4Ay"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_featurized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_featurized.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the data, as there is some correlation in the inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_featurized=df_featurized.sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ESQ7xKNFkUj9",
    "outputId": "72554bcf-a6de-46a8-f424-a1fab5394bfd"
   },
   "outputs": [],
   "source": [
    "# Splitting into train and test sets according to the original dataset\n",
    "y_train = df_featurized[df_featurized['Status'] =='Training']['EXP. Data']\n",
    "x_train = df_featurized[df_featurized['Status'] =='Training'].drop(['EXP. Data', 'HBD','HBD_smiles', 'Status', 'HBA', 'HBA_smiles'], axis=1)\n",
    "y_test = df_featurized[df_featurized['Status'] =='Test']['EXP. Data']\n",
    "x_test = df_featurized[df_featurized['Status'] =='Test'].drop(['EXP. Data', 'HBD','HBD_smiles', 'Status', 'HBA', 'HBA_smiles'], axis=1)\n",
    "print('train size: ',x_train.shape[0])\n",
    "print('test size: ',x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = tree.DecisionTreeRegressor(max_depth=3)\n",
    "dt_model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = dt_model.predict(x_train)\n",
    "y_pred_test = dt_model.predict(x_test)\n",
    "print(\"Train MSE=\",mean_squared_error(y_pred_train,y_train))\n",
    "print(\"Test MSE=\",mean_squared_error(y_pred_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=300)\n",
    "tree.plot_tree(dt_model,feature_names=x_train.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hacking**: look up some of the decision tree options, and see if you can do better!  Remember, you want to do better on the TEST MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FV4_vQJoqyJ2"
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(max_depth=5,\n",
    "                                 n_estimators=200,\n",
    "                                 min_samples_split=3,\n",
    "                                 min_samples_leaf=1)\n",
    "rf_model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = rf_model.predict(x_train)\n",
    "y_pred_test = rf_model.predict(x_test)\n",
    "print(\"Train MSE=\",mean_squared_error(y_pred_train,y_train))\n",
    "print(\"Test MSE=\",mean_squared_error(y_pred_test,y_test))\n",
    "print(\"Train MAE=\",mean_absolute_error(y_pred_train,y_train))\n",
    "print(\"Test MAE=\",mean_absolute_error(y_pred_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are we going to try to visualize these!  NO! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hacking**: Look up some of the additional random forest options, and see if you can do better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter search\n",
    "\n",
    "Let's try to automate the process above.  `scikit-learn` has code to do this!  It's called `GridSearchCV`, Grid, and it peforms cross-validation on a whole list of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XjMS3i3Gr8vd",
    "outputId": "e7359de5-a226-46ed-8650-3f84b8267637"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Hyperparameter tuning and cross validation\n",
    "param_grid = {\n",
    "    'n_estimators': [50,100],  # Number of trees\n",
    "    'max_depth': [5, 10, 20],      # Maximum depth of each tree\n",
    "    'max_features': [1.0, 'sqrt', 'log2'],    # Number of features to consider at each split\n",
    "    'min_samples_split': [2,3,4],  # Minimum number of samples required to split a node\n",
    "    'min_samples_leaf': [1,2,3]     # Minimum number of samples required at each leaf node\n",
    "}\n",
    "rf_model = RandomForestRegressor()  # model creation\n",
    "grid_search = GridSearchCV(rf_model,\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5,\n",
    "                           verbose=3,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True) # will go through all possible combinations in the param grid\n",
    "grid_search.fit(x_train,y_train) # fitting to train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J1c9AiM_qmDK",
    "outputId": "382e2110-4772-43f5-da60-e53fa5a1b707"
   },
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be pretty random!  Last time I ran this I got:\n",
    "```\n",
    "Best hyperparameters:  {'max_depth': 10, 'max_features': 1.0, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 100} ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "jipsNjhTsR-Q",
    "outputId": "5b5ed3cf-523e-44fe-952f-9c503864c61f"
   },
   "outputs": [],
   "source": [
    "# Now, let's see the performance on our held out test set\n",
    "rf_model = RandomForestRegressor(**grid_search.best_params_).fit(x_train,y_train)\n",
    "print('training score: ',r2_score(rf_model.predict(x_train),y_train).round(3))\n",
    "print('test score: ', r2_score(rf_model.predict(x_test),y_test).round(3))\n",
    "\n",
    "#parity plot\n",
    "plt.scatter(y_test, rf_model.predict(x_test))\n",
    "plt.plot(y_test,y_test)\n",
    "plt.text(65, 36, s ='r2 score: {}'.format(r2_score(rf_model.predict(x_test), y_test).round(3)))\n",
    "plt.text(65, 34, s ='MAE: {}'.format(mean_absolute_error(rf_model.predict(x_test), y_test).round(3)))\n",
    "plt.text(65, 32, s ='MSE: {}'.format(mean_squared_error(rf_model.predict(x_test), y_test).round(3)))\n",
    "plt.xlabel('Actual Value')\n",
    "_ = plt.ylabel('Predicted Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "VafU_T8-p3tK",
    "outputId": "b74231cc-e085-42c4-95bf-ec7ba730ff6c"
   },
   "outputs": [],
   "source": [
    "#Analizing most important features\n",
    "df_ft_imp_rf = pd.DataFrame({'feature': x_train.columns,'importance': rf_model.feature_importances_}).sort_values('importance',ascending=True)\n",
    "df_ft_imp_rf.tail(10).plot.barh('feature','importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHkdtvEkupZI"
   },
   "source": [
    "# Some parting words \n",
    "Some things to keep in mind about random forest methods\n",
    "*   It's a fairly robust model, and usually doesn't overfit\n",
    "*   As it's a tree based model, scaling of the data is not required\n",
    "*   It's not good at extrapolation, see here: https://www.kaggle.com/code/carlmcbrideellis/extrapolation-do-not-stray-out-of-the-forest\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTWq88zyvpfv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
