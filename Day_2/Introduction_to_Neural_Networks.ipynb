{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Neural Networks\n"
      ],
      "metadata": {
        "id": "LfXndeEB-Xy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/icomse/8th_workshop_MachineLearning.git\n",
        "import os\n",
        "os.chdir('8th_workshop_MachineLearning/data')"
      ],
      "metadata": {
        "id": "QSBvifcs_DeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHWcE_8d89Jr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import scipy.optimize as op\n",
        "import scipy.io as sio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "AmdAdcmSgqxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Read in data\n",
        "data = pd.read_csv('Exam_Data.txt')\n",
        "data.head()\n",
        "X = data.iloc[:,0:2]\n",
        "Y = data.iloc[:, 2]\n",
        "\n",
        "print(X.head())\n",
        "Y.head()\n",
        "\n",
        "X_new = X.copy()\n",
        "X_new.insert(0,'x0', np.ones(len(X_new.index)))\n",
        "print(X_new.head())"
      ],
      "metadata": {
        "id": "FOtubRW1-jcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Plot data. Colors indicate if a student passed a class or not.\n",
        "plt.scatter(X_new.iloc[:,1],X_new.iloc[:,2],c = Y)#,Y)\n",
        "plt.xlabel('Exam 1')\n",
        "plt.ylabel('Exam 2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "J459biQcByaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EGpI27vxQEY"
      },
      "source": [
        "We are going to use the sigmoid function for logistic regression. Code it up in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lvbqHPDYQPQO"
      },
      "outputs": [],
      "source": [
        "###Define sigmoid function\n",
        "def sigmoid(z):\n",
        "  '''Function that takes an input z and returns the sigmoid transformation.\n",
        "  If z is a vector or a matrix, it should perform the sigmoid transformation on every element.\n",
        "  Input:\n",
        "  z [=] scalar or array\n",
        "  Return:\n",
        "  g [=] scalar or array\n",
        "  '''\n",
        "\n",
        "  ###BEGIN SOLUTION\n",
        "\n",
        "\n",
        "  ###END SOLUTION\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWFdArLSRt0X"
      },
      "outputs": [],
      "source": [
        "##Test your function with different values and arrays.\n",
        "\n",
        "test1 = 100000\n",
        "test2 = -100000\n",
        "test3 = np.array([test1, test2, 0])\n",
        "\n",
        "print(sigmoid(test1), sigmoid(test2), sigmoid(test3))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code the cost function for logistic regression below."
      ],
      "metadata": {
        "id": "RheyeQsKGVFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cost(theta,X,y):\n",
        "  ''' Function to calculate cost function assuming a hypothesis of form h = 1/(1+exp(theta.T*X))\n",
        "  Inputs:\n",
        "  X = feature matrix (Up to developer whether or not it includes columns of 1, but recommend it does)\n",
        "  y = array of training examples\n",
        "  theta = array of parameters for hypothesis\n",
        "\n",
        "  Returns:\n",
        "  J = cost function\n",
        "  '''\n",
        "  ###BEGIN SOLUTION\n",
        "\n",
        "  ###END SOLUTION\n",
        "  return J"
      ],
      "metadata": {
        "id": "1mIYhsypCOsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Test cost function. If done properly, you should get a value close to 0.693\n",
        "\n",
        "initial_theta = np.zeros(len(X.columns)+1)\n",
        "print(initial_theta)\n",
        "###BEGIN SOLUTION\n",
        "\n",
        "###END SOLUTION"
      ],
      "metadata": {
        "id": "4GrpJv0nGejK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using advanced optimization algorithms require we provide them with a function that reports the gradient of the function we want to optimize. Write a function that returns the gradient of the cost function."
      ],
      "metadata": {
        "id": "uJvtWPvcIllL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Optimization algorithm\n",
        "#Define gradient of cost function\n",
        "\n",
        "###BEGIN SOLUTION\n",
        "\n",
        "###END SOLUTION"
      ],
      "metadata": {
        "id": "Ua6fI-GOGh32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Test your grad function You should expect: [ -0.1        -12.00921659 -11.26284221]\n",
        "G = Gradient(initial_theta, X_new,Y)\n",
        "print(G)"
      ],
      "metadata": {
        "id": "VJSHuPGuImU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now use the following command to perform the optimization. Expect values [-25.16131878   0.20623159   0.20147149]\n",
        "\n",
        "Result = op.minimize(fun = cost, x0=initial_theta, args = (X_new,Y), method='TNC', jac = Gradient)\n",
        "\n",
        "optimal_theta = Result.x\n",
        "print(optimal_theta)"
      ],
      "metadata": {
        "id": "JNgHXhG8Iq_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the optimal thetas for our logistic regression, replot the data along with the decision boundary."
      ],
      "metadata": {
        "id": "pi1t9AXhIvvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Replot data with decision boundary\n",
        "###BEGIN SOLUTION\n",
        "\n",
        "\n",
        "###END SOLUTION"
      ],
      "metadata": {
        "id": "hICHqviqI0Dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How accurate is your model?"
      ],
      "metadata": {
        "id": "xarvyxwHI4_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##What is the accuracy of the model?\n",
        "\n",
        "###BEGIN SOLUTION\n",
        "\n",
        "\n",
        "###END SOLUTION"
      ],
      "metadata": {
        "id": "SJl7nu8tI64-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data from the mnist database."
      ],
      "metadata": {
        "id": "nJcY2FnOOWtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load in data which comes in as a MATLAB matrix\n",
        "mat_contents = sio.loadmat(\"MNIST.mat\")"
      ],
      "metadata": {
        "id": "LmQfS-bHI83H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The matrix is read in as a dictionary, so we can split it up into variables using the appropriate keys. Each row of $X$ consists of the flattened 20x20 pixel grid of a grayscale image corresponding to a number. Each row of $y$ consists of the number that is represented by the pixels in a row of $X$.\n",
        "\n",
        "See below that we changed all values in $y$ that are $10$ to $0$; this is to account for some Matlab adjustments. This way when $i = 0, 1, 2, 3, ..., 9$ it matches the digit that is being predicted.\n",
        "\n",
        "We also set up a feature matrix."
      ],
      "metadata": {
        "id": "mAy7Fsc5Oehw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assign values to matrices\n",
        "X = mat_contents['X']\n",
        "y = mat_contents['y'].flatten()\n",
        "\n",
        "#Change y values that are 10 to 0.\n",
        "\n",
        "for i in range(len(y)):\n",
        "  if y[i] == 10:\n",
        "    y[i] = 0\n",
        "\n",
        "#Shape of X. Which one is rows and which one is columns?\n",
        "m,n = np.shape(X)\n",
        "\n",
        "#Vector of ones to add to feature matrix. What are our features in this case?\n",
        "ones = np.ones(m)\n",
        "\n",
        "#X_new is our feature matrix with the added column of ones.\n",
        "X_new = np.vstack((ones,X.T)).T\n"
      ],
      "metadata": {
        "id": "tYJisJ-jObI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we get to a neural network, it is helpful to build intuition using logistic regression to do our classification. To do so, we need our sigmoid function and cost function."
      ],
      "metadata": {
        "id": "tacUQxLTOodh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Vectorized version of logistic regression with regularization\n",
        "###Define sigmoid function\n",
        "def sigmoid(z):\n",
        "  '''Function that takes an input z and returns the sigmoid transformation.\n",
        "  If z is a vector or a matrix, it should perform the sigmoid transformation on every element.\n",
        "  Input:\n",
        "  z [=] scalar or array\n",
        "  Return:\n",
        "  g [=] scalar or array\n",
        "  '''\n",
        "\n",
        "  #\n",
        "  g = 1/(1+np.exp(-z))\n",
        "\n",
        "  return g\n",
        "\n",
        "\n",
        "def cost(theta,X,y,l):\n",
        "  ''' Function to calculate cost function assuming a hypothesis of form h = 1/(1+exp(theta.T*X))\n",
        "  Inputs:\n",
        "  X = feature matrix (Up to developer whether or not it includes columns of 1, but recommend it does)\n",
        "  y = array of training examples\n",
        "  theta = array of parameters for hypothesis\n",
        "  l = regularization parameter\n",
        "\n",
        "  Returns:\n",
        "  J = regularized cost function\n",
        "  '''\n",
        "\n",
        "  m = len(y) #\n",
        "\n",
        "  z= np.dot(X,theta) #\n",
        "  h = sigmoid(z)#\n",
        "  b = theta[1:]#\n",
        "\n",
        "  J = (1/m)*(-y@np.log(h) - (1-y)@np.log(1-h)) + l/(2*m)*b@b#\n",
        "  return J"
      ],
      "metadata": {
        "id": "Yfe38Fk_OjwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With our cost function now defined, we need to define the gradient of the cost function to use the advanced optimization algorithms. Complete the function below. Do not forget this is for the regularized version."
      ],
      "metadata": {
        "id": "ewn6givIb5v7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Gradient(theta, X,y, l):\n",
        "  '''Gradient of cost function\n",
        "    Inputs:\n",
        "  X = features\n",
        "  y = training data\n",
        "  theta = parameters\n",
        "  l = regularization parameter\n",
        "  Output:\n",
        "  grad = gradient of cost function\n",
        "  '''\n",
        "###BEGIN SOLUTION\n",
        "\n",
        "###END SOLUTION"
      ],
      "metadata": {
        "id": "ou3iVhGvOt52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test your implementation with the cell below."
      ],
      "metadata": {
        "id": "kz9Fglk-cCLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Test the various functions\n",
        "\n",
        "X_t = np.array(([1.0000,0.1000,0.6000,1.1000],\n",
        "   [1.0000,   0.2000,   0.7000,   1.2000],\n",
        "   [1.0000,   0.3000,   0.8000,   1.3000],\n",
        "   [1.0000,   0.4000,   0.9000,   1.4000],\n",
        "   [1.0000,   0.5000,   1.0000,   1.5000]))\n",
        "\n",
        "#print(X_t, X_t[0])\n",
        "\n",
        "y_t = np.array(([1, 0, 1, 0, 1]))\n",
        "#print(y_t)\n",
        "\n",
        "theta_t = np.array(([-2,-1,1,2]))\n",
        "print(cost(theta_t,X_t,y_t, 3))\n",
        "print(Gradient(theta_t,X_t,y_t,3))\n",
        "\n",
        "##If cost is correctly implemented, expect a value of 2.534819\n",
        "##If gradient is correctly implemented, expect values of [ 0.14656137 -0.54855841  0.72472227  1.39800296]"
      ],
      "metadata": {
        "id": "pnI08PKZb6iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the gradient set up, we can now perform a \"one vs. all\" classification algorithm to identify the numbers. Think carefully about the algorithm and read through the function below. What is it doing? Make sure to understand what each line is doing and add the appropriate comments."
      ],
      "metadata": {
        "id": "HCC-IGfWe-9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#One vs. all classification\n",
        "\n",
        "#With all your pieces in place, a one vs. all algorithm can be used to classify the different digits.\n",
        "#The function below gets us part of the way there.\n",
        "\n",
        "def onevsall(X,y,num_labels,l):\n",
        "  '''\n",
        "  Function that will train multiple logistic regression classifiers and returns\n",
        "  the optimal thetas associated with the labels\n",
        "\n",
        "  X = feature matrix\n",
        "  y = labeled data\n",
        "  num_labels = number of labels to be classified\n",
        "  l = regularization parameter\n",
        "  '''\n",
        "  m,n = np.shape(X)\n",
        "  all_theta = np.zeros((num_labels,n))\n",
        "\n",
        "  for i in range(num_labels):\n",
        "    initial_theta = np.zeros(n)\n",
        "    y_binary = np.zeros(m)\n",
        "    for j in range(m):\n",
        "      if y[j] ==i:\n",
        "        y_binary[j]=1\n",
        "    Result = op.minimize(fun = cost, x0=initial_theta, args = (X,y_binary,l), method='TNC', jac = Gradient)\n",
        "    optimal_theta = Result.x\n",
        "\n",
        "    all_theta[i,:] = optimal_theta\n",
        "\n",
        "  return all_theta"
      ],
      "metadata": {
        "id": "g3UrZYx_dD5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the function set up, we can use it and obtain some thetas. What are they and what do they represent for each label? What is the structure of theta?"
      ],
      "metadata": {
        "id": "wDRt00eRfC2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l_reg = 0.1\n",
        "\n",
        "theta = onevsall(X_new,y,10,l_reg)"
      ],
      "metadata": {
        "id": "pNsmfCy9fBus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now with the optimal thetas, write code below to find the accuracy of the algorithm."
      ],
      "metadata": {
        "id": "8m3XLgpufLOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Prediction\n",
        "\n",
        "###BEGIN SOLUTION\n",
        "\n",
        "###END SOLUTION"
      ],
      "metadata": {
        "id": "PLmQ7WktfIwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3_hqvtWffVeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Networks"
      ],
      "metadata": {
        "id": "Osm4xB9dgufV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have taken optimized thetas for a neural network and now need to do a forward propagation to do the predictions. Theta1 is the set of weights linking the input layer to the hidden layer. Theta2 is the set of weights linking the hidden layer to the output."
      ],
      "metadata": {
        "id": "oiAPCDX3htGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###Forward propagation with pretrained thetas\n",
        "weights = sio.loadmat(\"Weights.mat\")\n",
        "Theta1 = weights['Theta1']\n",
        "Theta2 = weights['Theta2']"
      ],
      "metadata": {
        "id": "jYfcLismgx4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform the forward propagation, calculate the prediction on the data and assess the accuracy."
      ],
      "metadata": {
        "id": "vt5Bn5PCrFSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Neural Network\n",
        "\n",
        "###BEGIN SOLUTION\n",
        "\n",
        "###END SOLUTION"
      ],
      "metadata": {
        "id": "ufNisMYponBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3L6U5c2irMRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backpropagation"
      ],
      "metadata": {
        "id": "jGB1_JEqxXbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Play close attention to the data structure."
      ],
      "metadata": {
        "id": "F6nPEcYtxvSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.shape(Theta1))\n",
        "m_t1, n_t1 = np.shape(Theta1)\n",
        "print(np.shape(Theta2))\n",
        "m_t2, n_t2 = np.shape(Theta2)\n",
        "\n",
        "##Combining Thetas into a single array\n",
        "Thetas = np.hstack([Theta1.flatten(), Theta2.flatten()])\n",
        "print(Thetas, len(Thetas))\n",
        "\n",
        "##Example of reshaping\n",
        "Theta_reshaped_1 = Thetas[:m_t1*n_t1].reshape(m_t1,n_t1)\n",
        "\n",
        "print(Theta_reshaped_1, np.shape(Theta_reshaped_1))\n",
        "\n",
        "Theta_reshaped_2 = Thetas[m_t1*n_t1:].reshape(m_t2, n_t2)\n",
        "\n",
        "print(np.shape(Theta_reshaped_2))"
      ],
      "metadata": {
        "id": "zq8neWMKxY7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a function that returns the cost function of a non regularized neural network. Recall that the output of a neural network is going to be a vector with zeros and ones, with the one at the position that corresponds to the digit to be classified. In this case, for a given datum y = 1, we have already changed it to y = [1,0,0...,0]. Similarly, for a datum of y = 10, changed it to y = [0,0,0,...,0,1].\n",
        "\n",
        "Also, think about how you want to take in the theta parameters. I recommend taking in a single array that contains all of the thetas and you rebuild it inside the function.\n",
        "\n",
        "For this case we have 400 nodes in our input layer and 25 nodes in the hidden layer. There are ten nodes in the output layer."
      ],
      "metadata": {
        "id": "GnALkBBCx3vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###Transforming y data to vectors for NN\n",
        "y_nn = y.copy()\n",
        "y_nn -=1\n",
        "y_matrix = np.eye(10)[y_nn[:]]\n",
        "print(y_matrix, y_matrix[0], len(y_matrix), len(y_matrix[0]))\n",
        "\n",
        "###BEGIN SOLUTION\n",
        "\n",
        "###END SOLUTION\n",
        "\n",
        "\n",
        "## If your function is correctly implemented, you should expect a cost value of 0.287629"
      ],
      "metadata": {
        "id": "w1lA1hA_xt1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now do the cost function again, but with regularization."
      ],
      "metadata": {
        "id": "TdRgOodKy0a1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###BEGIN SOLUTION\n",
        "###END SOLUTION\n",
        "\n",
        "## If your function is correctly implemented, you should expect a cost value of 0.383770"
      ],
      "metadata": {
        "id": "3IRqTIpdx_Oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to calculate gradients for backpropagation\n",
        "\n",
        "Set up function to calculate gradient of sigmoid. It should work with scalars or vectors."
      ],
      "metadata": {
        "id": "6oL42gD1y_jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###BEGIN SOLUTION\n",
        "\n",
        "###END SOLUTION"
      ],
      "metadata": {
        "id": "Eod6uXGvy4ZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For proper calculation of the thetas, you need to iterate over every datum, do forward propagation, and then calculate the deltas. Then accumulate into the gradient of cost functions. You can add regularization after the for loop over data.\n",
        "\n",
        "Set up a function that returns the flattened gradient for the thetas for a regularized cost function. Call it Gradient"
      ],
      "metadata": {
        "id": "TvsIfFpFzH2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##For proper calculation, you need to iterate over every datum, do forward propagation, and then calculate the deltas\n",
        "##Then accumulate into the gradient of cost functions. You can add regularization after the for loop over data.\n",
        "\n",
        "#Set up a function that returns the flattened gradient for the thetas. Call it Gradient\n",
        "###BEGIN SOLUTION\n",
        "\n",
        "###END SOLUTION\n",
        "\n",
        "#If done correctly, expect an output of the gradient like this: [ 6.18712766e-05 -2.11248326e-12  4.38829369e-13 ...  4.70513145e-05\n",
        "#-5.01718610e-04  5.07825789e-04]"
      ],
      "metadata": {
        "id": "EDO3sIK_zBrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now with our cost function and gradient function, we can use the advanced optimization algorithms to find the optimal thetas."
      ],
      "metadata": {
        "id": "nT_B1x3WzbMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rand_theta_1 = np.random.rand(25,401)*2*0.12-0.12\n",
        "rand_theta_2 = np.random.rand(10,26)*2*0.12-0.12\n",
        "\n",
        "initial_theta = np.hstack([rand_theta_1.flatten(), rand_theta_2.flatten()])\n",
        "\n",
        "\n",
        "Result = op.minimize(fun = cost_reg, x0=initial_theta, method='L-BFGS-B', jac = Gradient, args=(X_new,y_matrix,1,400,25,10),options={'maxiter':50})\n",
        "optimal_theta = Result.x\n",
        "\n",
        "print(optimal_theta)"
      ],
      "metadata": {
        "id": "rLz0yOdLzP_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate predictions based on your optimized thetas. If implemented correctly, expect accuracy close to 95%."
      ],
      "metadata": {
        "id": "Aq_GhmEYzlYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###BEGIN SOLUTION\n",
        "\n",
        "###END SOLUTION"
      ],
      "metadata": {
        "id": "by42oM0RzjKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using PyTorch"
      ],
      "metadata": {
        "id": "P-w-QrUu3H0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "## Code from https://github.com/rasbt/machine-learning-book/blob/main/ch12/ch12_part2.ipynb"
      ],
      "metadata": {
        "id": "4Snxbg7Qz8QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "X = iris['data']\n",
        "y = iris['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=1./3, random_state=1)"
      ],
      "metadata": {
        "id": "eR_tZI9eHGjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_norm = (X_train - np.mean(X_train)) / np.std(X_train)\n",
        "X_train_norm = torch.from_numpy(X_train_norm).float()\n",
        "y_train = torch.from_numpy(y_train)\n",
        "\n",
        "train_ds = TensorDataset(X_train_norm, y_train)\n",
        "\n",
        "torch.manual_seed(1)\n",
        "batch_size = 2\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "Rl7x-Ku1JbtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer2(x)\n",
        "        x = nn.Softmax(dim=1)(x)\n",
        "        return x\n",
        "\n",
        "input_size = X_train_norm.shape[1]\n",
        "hidden_size = 16\n",
        "output_size = 3\n",
        "\n",
        "model = Model(input_size, hidden_size, output_size)\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "x5p_yueEJdrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 100\n",
        "loss_hist = [0] * num_epochs\n",
        "accuracy_hist = [0] * num_epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    for x_batch, y_batch in train_dl:\n",
        "        pred = model(x_batch)\n",
        "        loss = loss_fn(pred, y_batch.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss_hist[epoch] += loss.item()*y_batch.size(0)\n",
        "        is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
        "        accuracy_hist[epoch] += is_correct.sum()\n",
        "\n",
        "    loss_hist[epoch] /= len(train_dl.dataset)\n",
        "    accuracy_hist[epoch] /= len(train_dl.dataset)"
      ],
      "metadata": {
        "id": "0b73ZR4pJfhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(12, 5))\n",
        "ax = fig.add_subplot(1, 2, 1)\n",
        "ax.plot(loss_hist, lw=3)\n",
        "ax.set_title('Training loss', size=15)\n",
        "ax.set_xlabel('Epoch', size=15)\n",
        "ax.tick_params(axis='both', which='major', labelsize=15)\n",
        "\n",
        "ax = fig.add_subplot(1, 2, 2)\n",
        "ax.plot(accuracy_hist, lw=3)\n",
        "ax.set_title('Training accuracy', size=15)\n",
        "ax.set_xlabel('Epoch', size=15)\n",
        "ax.tick_params(axis='both', which='major', labelsize=15)\n",
        "plt.tight_layout()\n",
        "\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0DzJbakQJhLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_norm = (X_test - np.mean(X_train)) / np.std(X_train)\n",
        "X_test_norm = torch.from_numpy(X_test_norm).float()\n",
        "y_test = torch.from_numpy(y_test)\n",
        "pred_test = model(X_test_norm)\n",
        "\n",
        "correct = (torch.argmax(pred_test, dim=1) == y_test).float()\n",
        "accuracy = correct.mean()\n",
        "\n",
        "print(f'Test Acc.: {accuracy:.4f}')"
      ],
      "metadata": {
        "id": "Immd4CIBJlIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y9uMj53AY3EQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}