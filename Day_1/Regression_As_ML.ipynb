{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, make sure the notebook is aware of the workshop data sets\n",
    "!git clone https://github.com/icomse/8th_workshop_MachineLearning.git\n",
    "import os\n",
    "os.chdir('8th_workshop_MachineLearning/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression as machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the simplest models we can! Plain old fitting to a line; the simplest model there can be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the names in sklearn tend to be pretty long, so we will import the individual\n",
    "# objects rather than the whole library \n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's read in some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is some artificial data we will be playing around with to illustrate some important concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlin = pd.read_csv('linmod.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What does this data look like? Inspect directly and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>outputs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.894</td>\n",
       "      <td>16.107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.691</td>\n",
       "      <td>16.509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.080</td>\n",
       "      <td>16.941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.344</td>\n",
       "      <td>17.382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.662</td>\n",
       "      <td>17.831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   inputs  outputs\n",
       "0   8.894   16.107\n",
       "1   8.691   16.509\n",
       "2  11.080   16.941\n",
       "3  16.344   17.382\n",
       "4  13.662   17.831"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dlin.plot(x='inputs',y='outputs',kind='scatter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick question: Why don't you see many graphs inspecting the data with machine learning studies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer: There are many different "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train a linear model with the data! First, create the model. It starts out as just an empty Python object that we need to fill provide data to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linmodel = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look up the documentation as to what to do to fit the model. When you fit it, what do you get back? \n",
    "\n",
    "NOTE: `LinearRegression` expects that that the input X is a 2D arrray, of size `[n_features, n_samples]`, because it is designed to handle the general case that you are doing multiple variable linear regression. So if you pass in a 1D array, it will error. Read the error message you get carefully for a quick fix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hacking**: Read the documentation and fit the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, you have a model. We next want to plot the prediction of the model and the points on the same plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = np.linspace(10,70,100)\n",
    "ypred = linmodel.predict(xvals.reshape(-1,1))  # use this to apply the model\n",
    "plt.plot(xvals,ypred,c='k')\n",
    "plt.scatter(X,Y)\n",
    "plt.xlabel('inputs')\n",
    "plt.ylabel('outputs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What methods does the `LinearRegression` have? What information can you get from it?  What are the parameters of a 1D linear regression?  What is a measure of how good the model is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linmodel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, let's look at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linmodel.intercept_)\n",
    "print(linmodel.coef_)\n",
    "print(linmodel.score(X,Y))  # in this case, the score gives the R^2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play around with (over)fitting!  There's a little bit of curvature - in the data; maybe we should try a polynomial fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go off and do more complicated models, let's actually just use a **subset** of the data set to train. Then, if we have reserved some data, we can test afterwards if our model does a good job fitting all of the data, or specifically fit the data we select.  There are tools doing this type of split!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a polynomial fit using sklearn by taking powers of the input features.  `scikit-learn` has a function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use it, you create a `PolynomialFeatures` object of the degree you are interested in, then take the powers of the data using the fit_transform method of `PolynomialFeatures`. See example (you will have to supply your own X, which is the same input as `LinearRegression.fit` takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = PolynomialFeatures(degree=2)\n",
    "pX_train = pf.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[:5])\n",
    "print(pX_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it's a degree $\\times$ n_data array; with each column the input data to power $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets see how the model fits the data.  Inspect the model parameters and the $R^2$, and plot the fit and the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linmodel.fit(pX_train,Y_train)\n",
    "print(linmodel.intercept_)\n",
    "print(linmodel.coef_)\n",
    "print('R^2 (train) =', linmodel.score(pX_train,Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = np.linspace(10,70,100)\n",
    "px = pf.fit_transform(xvals.reshape(-1,1))\n",
    "ypredict = linmodel.predict(px)\n",
    "plt.plot(xvals,ypredict,c='k')\n",
    "plt.scatter(X_train,Y_train)\n",
    "plt.xlabel('inputs')\n",
    "plt.ylabel('outputs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pX = pf.fit_transform(X_test)\n",
    "plt.plot(xvals,ypredict,c='k')\n",
    "plt.scatter(X_test,Y_test)\n",
    "plt.xlabel('inputs')\n",
    "plt.ylabel('outputs')\n",
    "print('R^2 =', linmodel.score(pX,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, it's the TESTING MSE (or $R^2$) that we want to minimize, not the training MSE.  If the training MSE is low, but the testing is high, then the model is overfit.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HACKING TIME**: Now, go back and try a higher polynomial degree. Note that polynomial fitting is not very numerically stable, so if you fit to too many points, then you start to have problems.  What problems do you have?\n",
    "\n",
    "Can you plot $R^2$ (i.e. linmodel.score) for the test and training set versus polynomial degree?  What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = PolynomialFeatures(degree=4) \n",
    "pX_train = pf.fit_transform(X_train) # transform the features into a x^0 . . . x^15\n",
    "linmodel.fit(pX_train,Y_train) # train the model on the data\n",
    "print('R^2 (train) =', linmodel.score(pX_train,Y_train))\n",
    "pxvals = pf.fit_transform(xvals.reshape(-1,1)) # generate the polynomial input data\n",
    "ypredict = linmodel.predict(pxvals) # predict the curve of the model.\n",
    "plt.plot(xvals,ypredict,c='k') \n",
    "plt.scatter(X_test,Y_test,label='test',alpha=0.5)\n",
    "plt.scatter(X_train,Y_train,label='train',alpha=0.5)\n",
    "plt.legend()\n",
    "pX_test = pf.fit_transform(X_test)  # what is the transformed test data, so we can compute the score\n",
    "print('R^2 (test) =', linmodel.score(pX_test,Y_test)) # how well does the test data fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2test = list()\n",
    "R2train = list()\n",
    "ps = list()\n",
    "for p in range(2,20):\n",
    "    pf = PolynomialFeatures(degree=p)\n",
    "    pX_train = pf.fit_transform(X_train)\n",
    "    linmodel.fit(pX_train,Y_train)\n",
    "    R2train.append(linmodel.score(pX_train,Y_train))\n",
    "    pxvals = pf.fit_transform(xvals.reshape(-1,1)) # generate the polynomial input data\n",
    "    ypredict = linmodel.predict(pxvals)\n",
    "    pX_test = pf.fit_transform(X_test)\n",
    "    R2test.append(linmodel.score(pX_test,Y_test))\n",
    "    ps.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ps,R2test,label='test set')\n",
    "plt.plot(ps,R2train,label='training set')\n",
    "plt.xlabel('polynomial degree')\n",
    "plt.ylabel('$R^2$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here what is observed more generally: the training set will get a bit better on the same data as the model gets more complex, whereas the test model gets worse as the trained model starts to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilinear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's well and good, but that is just one input.  Usually, we have LOTS of features and want to use all of them to make our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cld=pd.read_csv('HCEPDB_100K_cleaned.csv') # change where it is\n",
    "# (but first you might refresh yourself on what is contained\n",
    "# we'll use a smaller sample to make it go faster, and to make the problem a bit harder.\n",
    "cld = cld[cld['pce']!=0] # clean out the data where PCE = 0 (not obtained)\n",
    "cld_sample = cld.sample(n=5000,random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic principle (and code!) is the same as ordinary least squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['mass', 'voc', 'jsc', 'e_homo_alpha', 'e_gap_alpha', 'e_lumo_alpha']\n",
    "observable = 'pce'\n",
    "X = cld_sample[features]\n",
    "Y = cld_sample[observable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now fit the model\n",
    "linmodel = LinearRegression() \n",
    "linmodel.fit(X,Y)\n",
    "linmodel.score(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there are more coefficients! One for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linmodel.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make a test-train split. We'll reserve most of the data for testing (we want to make it harder to fit!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(cld_sample, test_size=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One think to take into account, though. What if the data has different units, or is different types?  How will the coefficients change? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use data standardization or normalization to remove this effect of choice of units.  Now, it's not always the best thing to do.  If you have two variables that are the same units, it could be that the distances DO matter.  \n",
    "\n",
    "Basically, what we are saying is that the only real information in the input data is how many standard deviations away from the mean it is - we don't care about the absolute mangitude.\n",
    "\n",
    "Minimum, it makes sure that the model does not depend on the units used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(cld_sample[features])\n",
    "train_standardized = pd.DataFrame(data=scaler.transform(train[features]),columns=features)\n",
    "test_standardized = pd.DataFrame(data=scaler.transform(test[features]),columns=features)\n",
    "# stick back in the unscaled observables\n",
    "test_standardized['pce']=test['pce'].values \n",
    "train_standardized['pce']=train['pce'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For lasso and ridge regression we will look at soon, $R^2$ doesn't quite make sense, because the extra terms messes with calculation of the total sum of squares.  Instead, let's just look at the means squared error which depends just on the difference between predictions and the true data. \n",
    "\n",
    "Also, the coefficients of simple linear regression can take into account the change of units (it's the shrinkage term that makes the scale matter), but we'll do everything with standardized data to make it simpler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Just getting it set up with more rigor: Multiple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train linear model \n",
    "MLR=LinearRegression()\n",
    "MLR.fit(train[features],train[[observable]])\n",
    "\n",
    "# make predictions on test and train set \n",
    "trainpred=MLR.predict(train[features])\n",
    "testpred=MLR.predict(test[features])\n",
    "\n",
    "#make parity plot \n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(test[observable],testpred,color='r', label='Test')\n",
    "plt.scatter(train[observable],trainpred, label='Training')\n",
    "plt.plot(lw=4,color='black')\n",
    "plt.legend()\n",
    "plt.xlabel('Actual Output')\n",
    "plt.ylabel('Predicted Output')\n",
    "\n",
    "#calculate the test and train error\n",
    "print(\"Train error\",mean_squared_error(train[observable],trainpred))\n",
    "print(\"Test error\",mean_squared_error(test[observable],testpred))\n",
    "print(MLR.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Ridge Regression (same data as Part 1)\n",
    "\n",
    "* The ridge coefficients minimize $RSS + \\lambda \\sum_{j=1}^{p}\\beta_j^2$\n",
    "* There is an additional **penalty** in error for having large coefficients!\n",
    "* Note: ISLR shows the tuning parameter as $\\lambda$, but it is $\\alpha$ in `sk-learn`\n",
    "* Goal here: train models as a function of the regularization parameter \n",
    "* The X's **must** be be standardized for ridge regression to work correctly \n",
    "* Some methods in `sk_learn` also do automatic selection of shrinkage coefficient - you can try those out afterwards. \n",
    "* **For the next section, I suggest on your own you test out what normalization in ridge does, by executing the following code first WITHOUT and then WITH normalization** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of a single instance of ridge regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge=Ridge()\n",
    "a=2.0\n",
    "ridge.set_params(alpha=a)\n",
    "ridge.fit(train_standardized[features],train_standardized[observable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict = ridge.predict(train_standardized[features])\n",
    "test_predict = ridge.predict(test_standardized[features])\n",
    "print(mean_squared_error(train_standardized[observable],train_predict))\n",
    "print(mean_squared_error(test_standardized[observable],test_predict))\n",
    "print(ridge.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge=Ridge()\n",
    "a=8.0\n",
    "ridge.set_params(alpha=a)\n",
    "ridge.fit(train_standardized[features],train_standardized[observable])\n",
    "train_predict = ridge.predict(train_standardized[features])\n",
    "test_predict = ridge.predict(test_standardized[features])\n",
    "print(mean_squared_error(train_standardized[observable],train_predict))\n",
    "print(mean_squared_error(test_standardized[observable],test_predict))\n",
    "print(ridge.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "$\\alpha$ (or $\\lambda$) is an example of a *hyperparameter*.  It's a variable that will change exactly what the model is doing.  The parameters of the models are the coefficients of the ridge regression; each choice of parameter gives a different model, but each choice of hyperparameters leads to a different model. Deep learning methods often have *many* hyperparameters. Tuning hyperparameters is a key part of building machine learning models.  So let's tune a hyperparameter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hacking**: \n",
    "Vary $\\alpha$ and see how the predictions and coefficients change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(train_standardized[observable],train_predict, label='Training')\n",
    "plt.scatter(test_standardized[observable],test_predict,color='r', label='Test')\n",
    "plt.plot([-2.5,10],[-2.5,10],lw=4,color='black')\n",
    "plt.legend()\n",
    "plt.xlabel('Actual Output')\n",
    "plt.ylabel('Predicted Output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of searching the $\\alpha$ ($\\lambda$ in the notes) space in Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RR vs lambda (based on sklearn tutorial)\n",
    "coefs = []\n",
    "trainerror = []\n",
    "testerror = []\n",
    "\n",
    "# do you know what is happening here? \n",
    "model = Ridge()\n",
    "\n",
    "lambdas = np.logspace(-4,5,100)\n",
    "# loop over lambda values (strength of regularization and find the errors)\n",
    "for l in lambdas:\n",
    "    model.set_params(alpha=l)\n",
    "    model.fit(train_standardized[features],train_standardized[observable])\n",
    "    coefs.append(model.coef_)\n",
    "    trainerror.append(mean_squared_error(train_standardized[observable],model.predict(\n",
    "        train_standardized[features])))\n",
    "    testerror.append(mean_squared_error(test_standardized[observable],model.predict(\n",
    "        test_standardized[features])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is being plotted here? \n",
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(121)\n",
    "plt.plot(lambdas,coefs)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('coefs')\n",
    "plt.title('RR coefs vs $\\lambda$')\n",
    "plt.subplot(122)\n",
    "plt.plot(lambdas,trainerror,label='train error')\n",
    "plt.plot(lambdas,testerror,label='test error')\n",
    "plt.xscale('log')\n",
    "plt.ylim([0.01,1.5])\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('error')\n",
    "plt.legend(loc=1)\n",
    "plt.title('error vs $\\lambda$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pause and discuss with a partner**: What is happening in these plots? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is best (least overfit, best test MSE) at between $\\lambda$ ($\\alpha$) = $10^{-1}$ and $10$, specifically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas[np.argmin(testerror)] #the lambda where the MSE is minimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: LASSO regression  (same data as Part 1)\n",
    "\n",
    "* The LASSO improves over ridge regression by also providing a variable selection tool!\n",
    "* The LASSO minimizer is $RSS + \\lambda \\sum_{j=1}^{p}\\lvert\\beta_j\\rvert$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hacking time**: Plot the LASSO regression coefficients versus the magnitude of the shrinkage term, in the same way the ridge regression was done above, and find what values minimize the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see differently about the graphs with LASSO instead of ridge regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the coefficients are zero! Specifically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(features)[coefs[np.argmin(testerror)]==0.0]  # need to convert to numpy array to use masking. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So LASSO suggests that the best model would have just:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(features)[coefs[np.argmin(testerror)]!=0.0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### General linear models \n",
    "\n",
    "In the last class, we generated model based on the hypothesis that the $PCE$ of a candidate organic photovoltaic can be modeled as a contribution of the molecule's $mass$, $VOC$ and $E_{LUMO}$ values:  $PCE = \\beta_0 + \\beta_1\\times mass + \\beta_2\\times VOC + \\beta_3 \\times E_{LUMO}$\n",
    "\n",
    "The extension from multiple linear regression to general additive models is straightforward; it uses the same module `LinearRegression` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at the residuals, versus the function, and then the residuals versus each of the variables, and see if there is any nonlinearity. Check the RSE of this model for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(cld, test_size=0.95)\n",
    "features = ['mass', 'voc', 'e_lumo_alpha']\n",
    "observable = 'pce'\n",
    "train_X = train[features]\n",
    "train_Y = train[['pce']]\n",
    "test_X = test[features]\n",
    "test_Y = test[['pce']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression() \n",
    "model.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RSE =\",mean_squared_error(test_Y,model.predict(test_X)))\n",
    "plt.hexbin(test_Y,predictions-test_Y,cmap='hot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What sort of nonlinear model do these plots suggest to try?  Try something, and compare $R^2$, RSE, and plot the residuals of the new model and the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for complicated reasons, we have to add columns to the origina dataframe , not the cleaned dataframe \n",
    "# (basically, since the cleaned file is a slice of a dataframe, not a dataframe itself)\n",
    "cld['voc2'] = cld['voc']**2  \n",
    "cld['lumo2'] = cld['e_lumo_alpha']**2\n",
    "cld['voc3'] = cld['voc']**3\n",
    "cld['lumo3'] = cld['e_lumo_alpha']**3\n",
    "cld = cld[cld['pce']!=0]  # clean out bad PCE's again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(cld, test_size=0.95)\n",
    "features = ['mass','voc','voc2','voc3','e_lumo_alpha','lumo2','lumo3']\n",
    "observable = 'pce'\n",
    "train_X = train[features]\n",
    "train_Y = train[['pce']]\n",
    "test_X = test[features]\n",
    "test_Y = test[['pce']]\n",
    "\n",
    "model = LinearRegression() \n",
    "model.fit(train_X,train_Y)\n",
    "\n",
    "print(\"RSE =\",mean_squared_error(test_Y,model.predict(test_X)))\n",
    "plt.hexbin(test_Y,model.predict(test_X)-test_Y,cmap='hot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Hacking time!**: What other general linear models, more complicated or simpler, can you test? How do you know if they are better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just add one cross-term to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Logistic regression\n",
    "\n",
    "Let's use a database of perovskite stability data patients to demonstrate logistic regression.  Perovskites have the general formula $ABX_3$, where $A$ and $B$ are cations and $X$ is an anion.\n",
    "\n",
    "The variables are described as follows:\n",
    "- ABX3: chemical formula of the compound\n",
    "- exp_label: is it stable?  -1 is no, 1 is yes\n",
    "- is_train: is it training data? -1 is no, 1 is yes\n",
    "- nA: $n_A$, oxiation state of A\n",
    "- nB: $n_B$, oxidation state of B\n",
    "- nX: $n_X$, oxidations state of X\n",
    "- rA (Ang):\t$r_A$, ionic radius of A in Angstroms\n",
    "- rB (Ang):\t$r_B$, ionic radius of B in Angstroms\n",
    "- rX (Ang):\t$r_X$, ionic radius of X in Angstroms\n",
    "- t: $t$, Goldschmidt tolerance factor, $\\frac{r_A + r_X}{\\sqrt{2}\\left(r_B+r_X\\right)}$\n",
    "- tau: $\\tau$, Bartel et al. tolerance factor $\\frac{r_X}{r_B}-n_A\\left(n_A - \\frac{r_A/r_B}{\\ln(r_A/r_B)}\\right)$\n",
    "- t_pred: Whether $t$ predicts it will be a perovskite, -1 is no, 1 is yes\n",
    "- tau_pred:\tWhether $\\tau$ predicts it will be a perovskite ($\\tau < 4.18$), -1 is no, 1 is yes\n",
    "- tau_prob:\tProbability that the compund it is a perovskite, according to $\\tau$\t\n",
    "\n",
    "nA,nB,nX,rA,rB,rX are the features used for training.\n",
    "\n",
    "Let's start by just looking at the means and standard deviations to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perov=pd.read_csv('perovskite_data.csv')\n",
    "perov.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the input features which are not categorical. \n",
    "training_features = ['nA','nB','nX','rA (Ang)','rB (Ang)', 'rX (Ang)']\n",
    "table1=np.mean(perov[training_features],axis=0)\n",
    "table2=np.std(perov[training_features],axis=0)\n",
    "df = pd.DataFrame(columns=['means','stds'])\n",
    "df['means'] = table1\n",
    "df['stds'] = table2\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much is training versus validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perov['is_train'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LogisticRegression` is in `sklearn.linear_model` and has many of the same inputs and outputs as `LinearRegression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a `LogisticRegression` model on the six inputs, and score it. (What is the score here? is it $R^2$)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perov_training = perov[perov['is_train']==1]\n",
    "perov_testing =  perov[perov['is_train']==-1]\n",
    "inputData = perov_training[['nA','nB','nX','rA (Ang)','rB (Ang)', 'rX (Ang)']]\n",
    "outputData = (perov_training['exp_label']+1)/2  # rescale to 0,1, though it ends up not mattering.\n",
    "\n",
    "inputData_test = perov_testing[['nA','nB','nX','rA (Ang)','rB (Ang)', 'rX (Ang)']]\n",
    "outputData_test = (perov_testing['exp_label']+1)/2  # rescale to 0,1, though it ends up not mattering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit1=LogisticRegression(solver='liblinear')\n",
    "#we specify the liblinear solver to avoid a warning about not specifying\n",
    "logit1.fit(inputData,outputData)\n",
    "logit1.score(inputData,outputData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit1.fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Score` for logistic regression is just the fraction that are right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(logit1.predict(inputData)==outputData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit1.score(inputData,outputData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to use NO inputs, just the outputs, we would predict that the percent that are stable would be the same as the percent in our population, or about 50/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(outputData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are getting noticably better than random!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the documentation for the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model performance\n",
    "What are the percent of peroskevites thatare correctly classified by this model?  \n",
    "\n",
    "When we fail to predict correctly what are the possibilities?  For example, what are the true positive, true negative, false positive, and false negative rates?  Try using the `confusion_matrix` function of `sklearn.metrics`. How well does them model predict stability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(logit1.predict(inputData),outputData)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format is: \n",
    "[[ True Negative  False Negative],\n",
    " [ False Positive True Positive]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at coefficients.  Can you tell which ones are most important in predicting stability?  Why or why not? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit1.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "inputDataScaled = scaler.fit_transform(inputData)\n",
    "# just to inspect it better, transform it back into a pandas array\n",
    "inputDataScaled = pd.DataFrame(data=inputDataScaled,columns = inputData.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDataScaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hacking Time:** How does a prediction with normalized variables do versus one without normalized variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "All coefficients are now relatively similar. Maybe $n_B$ is not important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Is all of the data linearly independent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['nA','nB','rA (Ang)','rB (Ang)', 'rX (Ang)']\n",
    "inputData = perov_training[features]\n",
    "outputData = (perov_training['exp_label']+1)/2  # rescale to 0,1, though it ends up not mattering.\n",
    "inputData_test = perov_testing[features]\n",
    "outputData_test = (perov_testing['exp_label']+1)/2  # rescale to 0,1, though it ends up not mattering.\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "inputDataScaled = scaler.fit_transform(inputData)\n",
    "\n",
    "# just to inspect it better, transform it back into a pandas array\n",
    "inputDataScaled = pd.DataFrame(data=inputDataScaled,columns = inputData.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we transform using the fit from the _training_ data\n",
    "inputDataScaled_test = scaler.transform(inputData_test)  \n",
    "inputDataScaled_test = pd.DataFrame(data=inputDataScaled_test, columns = inputData.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit1=LogisticRegression(solver='liblinear')\n",
    "#we specify the liblinear solver to avoid a warning about not specifying\n",
    "logit1.fit(inputDataScaled,outputData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit1.score(inputDataScaled_test,outputData_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix((perov_training['tau_pred']+1)/2,outputData)\n",
    "print(cm)\n",
    "cm = confusion_matrix((perov_testing['tau_pred']+1)/2,outputData_test)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
