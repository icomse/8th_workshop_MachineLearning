{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/icomse/8th_workshop_MachineLearning.git\n",
    "import os\n",
    "os.chdir('8th_workshop_MachineLearning/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cld=pd.read_csv('../data/HCEPDB_100K_cleaned.csv') # change where it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(cld['pce'],kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm.  Looks like those PCE entries at zero are probably errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(cld['jsc'],kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cld = cld[cld['pce']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(cld['pce'],kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should be able to make more robust predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a range of different ways to impute data in both scikit learn and pandas. The cleanest data to input is when the data is arranged in some increasing array, or in some timeseries; then one can simply interpolate the missing numbers, i.e. putting in a number that is between it's neighbors.    \n",
    "\n",
    "However, in data set, there's not a clear ordering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cld=pd.read_csv('../data/HCEPDB_100K_cleaned.csv') # change where it is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data inputation works on values like `np.nan`, so we will replace the 0's with `NaN`'s. `inplace` mans in the current dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values=0.0, strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed = imp.fit_transform(cld[['pce','jsc']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cld[['pce','jsc']] = imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(cld['pce'],kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some more complex imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cld=pd.read_csv('../data/HCEPDB_100K_cleaned.csv') # change where it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cld.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_cluster = KNNImputer(missing_values=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_cols = ['pce','jsc','mass','e_homo_alpha','e_lumo_alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_cluster.fit(cld[fit_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_cluster.transform(cld[fit_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cld[fit_cols] = imp_cluster.transform(cld[fit_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(cld['pce'],kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another cleaning example\n",
    "Let's try cleaning some real-life Excel data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "Tzjy2CdEiuNK",
    "outputId": "d072ef52-a065-45c9-f228-ab226d60e68d"
   },
   "outputs": [],
   "source": [
    "# loading chlorine dataset\n",
    "df_cl = pd.read_excel('../Data/DES_SurfaceTension.xlsx', sheet_name='model1 (QSPR Cl)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "Tzjy2CdEiuNK",
    "outputId": "d072ef52-a065-45c9-f228-ab226d60e68d"
   },
   "outputs": [],
   "source": [
    "# We can see that our preferred column headers are on the 3rd column (index no.2) of the dataframe. Let's rename our column headers.\n",
    "df_cl = df_cl.rename(columns= dict(df_cl.iloc[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "Tzjy2CdEiuNK",
    "outputId": "d072ef52-a065-45c9-f228-ab226d60e68d"
   },
   "outputs": [],
   "source": [
    "# Our data starts from 3rd row, look at the No. column. Let's start from No. = 1\n",
    "df_cl = df_cl[3:]\n",
    "df_cl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "Tzjy2CdEiuNK",
    "outputId": "d072ef52-a065-45c9-f228-ab226d60e68d"
   },
   "outputs": [],
   "source": [
    "# Let's use the No. column as our index\n",
    "df_cl = df_cl.set_index('No.')\n",
    "df_cl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "Tzjy2CdEiuNK",
    "outputId": "d072ef52-a065-45c9-f228-ab226d60e68d"
   },
   "outputs": [],
   "source": [
    "# We see a lot of NaN values, they are basically blank cells in the excel sheet that we loaded. Let's get rid of them step by step.\n",
    "\n",
    "# First let's aim for columns where all the values are NaN\n",
    "df_cl = df_cl.dropna(axis = 1,how='all')\n",
    "df_cl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "Tzjy2CdEiuNK",
    "outputId": "d072ef52-a065-45c9-f228-ab226d60e68d"
   },
   "outputs": [],
   "source": [
    "# Now let's remove rows that have NaN values in any of it's cells. There should be no NaN in our dataframe\n",
    "df_cl = df_cl.dropna(axis = 0,how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "Tzjy2CdEiuNK",
    "outputId": "d072ef52-a065-45c9-f228-ab226d60e68d"
   },
   "outputs": [],
   "source": [
    "# Wait, there's a NaN in our column header. This column is a part of HBA, so let's add this to HBA and get rid the NaN column.\n",
    "df_cl.iloc[:,0] = df_cl.iloc[:,0] + df_cl.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "Tzjy2CdEiuNK",
    "outputId": "d072ef52-a065-45c9-f228-ab226d60e68d"
   },
   "outputs": [],
   "source": [
    "# But how do we say get rid of NaN column? NaN is not a string. What we can do is access it through the numpy library.\n",
    "\n",
    "df_cl = df_cl.drop(columns=[np.nan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "Tzjy2CdEiuNK",
    "outputId": "d072ef52-a065-45c9-f228-ab226d60e68d"
   },
   "outputs": [],
   "source": [
    "# The dataset also contains some pre calculated descriptors, predictions, standard deviations, etc. But we only need some of them.\n",
    "columns_to_keep = ['HBA', 'HBD', 'EXP. Data', 'Status', 'HBA:HBD']\n",
    "df_cl.columns = [x.strip() for x in df_cl.columns] # to get rid of possible whitespaces in the column name\n",
    "df_cl = df_cl[columns_to_keep]\n",
    "df_cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "zRwdkMgXju9y",
    "outputId": "c3935fbb-5e4b-430b-d327-cb00045a2668"
   },
   "source": [
    "Now, let's load the Bromine dataset. Thankfully they are organized similarly to the clorine dataset. So, we can just reuse our previous codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "zRwdkMgXju9y",
    "outputId": "c3935fbb-5e4b-430b-d327-cb00045a2668"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_br = pd.read_excel('DES_SurfaceTension.xlsx', sheet_name='model2 (QSPR Br)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "3YCUZh2-648C",
    "outputId": "b37ff62a-934d-40cd-ceb6-f42ae4b42f3c"
   },
   "outputs": [],
   "source": [
    "# Let's add these two datasets\n",
    "df = pd.concat([df_cl, df_br], ignore_index = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "LuliRKlUK7TM",
    "outputId": "4531d16c-2d63-40ac-c0d7-587fd4942b49"
   },
   "outputs": [],
   "source": [
    "#Let's check if there are any duplicated rows\n",
    "df[df.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "LuliRKlUK7TM",
    "outputId": "4531d16c-2d63-40ac-c0d7-587fd4942b49"
   },
   "outputs": [],
   "source": [
    "#There are some, let's remove them\n",
    "df=df.drop_duplicates()\n",
    "df = df.reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVMFnd2s_Ie4"
   },
   "source": [
    "### additional data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 831
    },
    "id": "7SI3r9FR5mvf",
    "outputId": "ed9fc891-3aae-4ce3-c820-4aca18d70817"
   },
   "outputs": [],
   "source": [
    "# Let's start with HBAs\n",
    "df['HBA'] = df['HBA'].str.lower() #First, let's make them all smaller case to make them case insensitive\n",
    "df['HBA'] = df['HBA'].str.split(' ').str.join('') #first, we'll get rid of all the spaces and make them one word\n",
    "df['HBA'] = df['HBA'].str.replace('chloride', ' chloride').str.replace('bromide', ' bromide') # now, let's give cation and anion a space\n",
    "df['HBA'] = df['HBA'].str.replace('-','') # hyphens are causing inconsistencies, let's just get rid of them\n",
    "#Okay our dataset improved a lot. However, there are still some typos, but it's really difficult to find a pattern here. As it's a small dataset, we can fix them by hardcoding.\n",
    "df = df.replace({'methyltriphenylphosphium bromide' : 'methyltriphenylphosphonium bromide',                 # Hardcoding to correct some typos\n",
    "                 'methyltriphenylphosphunium bromide' : 'methyltriphenylphosphonium bromide',\n",
    "                 'n,ndiethylethanolammonium chloride' : 'n,n-diethylethanolammonium chloride',\n",
    "                  'nbutyltriphenylphosphonium bromide' : 'n-butyltriphenylphosphonium bromide'})\n",
    "\n",
    "# we'll do similar works for HBDs\n",
    "df['HBD'] = df['HBD'].str.lower()\n",
    "df['HBD'] = df['HBD'].str.split(' ').str.join('')\n",
    "# we'll hardcode to correct some pesky typos. Some of them like space before acid and no space before aciddihydrate may not make sense now but it's required for SMILES conversion\n",
    "df['HBD'] = df['HBD'].str.replace('acid',' acid').str.replace(' aciddihydrate','aciddihydrate').replace('1,4butanediol', '1,4-butanediol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 831
    },
    "id": "7SI3r9FR5mvf",
    "outputId": "ed9fc891-3aae-4ce3-c820-4aca18d70817",
    "scrolled": true
   },
   "source": [
    "Finally! Let's look at the distribution of HBAs and HBDs in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 831
    },
    "id": "7SI3r9FR5mvf",
    "outputId": "ed9fc891-3aae-4ce3-c820-4aca18d70817",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_HBA = df['HBA'].value_counts()\n",
    "unique_HBD = df['HBD'].value_counts()\n",
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(16,8), dpi=200)\n",
    "ax1.barh(unique_HBA.index, unique_HBA.values)\n",
    "ax1.set_title(\"HBA distribution\")\n",
    "ax2.barh(unique_HBD.index, unique_HBD.values)\n",
    "ax2.set_title(\"HBD distribution\")\n",
    "# We can see the repetition of some components due to representational differences and typos. Let's fix them. (Go up and uncomment those lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
