{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9VKvsLG8bus"
      },
      "source": [
        "# Active Learning with Gaussian Processes\n"
      ],
      "id": "K9VKvsLG8bus"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3648340"
      },
      "source": [
        "Let's start by getting some intuition for Multivariate Gaussians. In the following, we will draw 5 random samples for a two-dimensional Gaussian."
      ],
      "id": "b3648340"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8a5186b"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "rng = np.random.default_rng() # sets up a random number generator\n",
        "ndim= 2\n",
        "nsamples = 5\n",
        "\n",
        "# define the distribution and draw samples\n",
        "mean=np.zeros(ndim) # zero mean\n",
        "cov =np.array([[1,0.8],[0.8,2]]) # some covariance matrix with correlation present\n",
        "samples = rng.multivariate_normal(mean,cov,nsamples)\n",
        "\n",
        "# for reference, we will generate a ton of samples to visualize the \"true\" distribution\n",
        "x1t,x2t = rng.multivariate_normal(mean,cov,1000000).T\n",
        "fig,ax = plt.subplots(1,2)\n",
        "ax[0].hist2d(x1t,x2t,bins=(100,100),cmap=plt.cm.magma_r)\n",
        "\n",
        "# and we will overlay our samples on top of this\n",
        "mycmap = plt.cm.get_cmap('winter',nsamples)\n",
        "myColors = [mycmap(i/(nsamples-1.)) for i in range(nsamples)]\n",
        "for i,sample in enumerate(samples):\n",
        "  ax[0].plot(sample[0],sample[1],linestyle = 'none', marker = 'x', color = myColors[i],linewidth=5 )\n",
        "ax[0].axis('square')\n",
        "ax[0].set_xlim([-6,6])\n",
        "ax[0].set_ylim([-6,6])\n",
        "\n",
        "# Now we will also represent these same draws by plotting them\n",
        "# as lines connecting the two Gaussian vectors\n",
        "xrvs = [dim/(ndim-1.) for dim in range(ndim)]\n",
        "for i,sample in enumerate(samples):\n",
        "    ax[1].plot(xrvs,sample,linestyle='-',marker = 'x',color=myColors[i],linewidth=2)\n",
        "ax[1].xaxis.set_ticks(xrvs)\n",
        "ax[1].xaxis.set_ticklabels([])\n",
        "plt.show()"
      ],
      "id": "f8a5186b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ad0b487"
      },
      "source": [
        "Now, we will follow this up for a more complex situation of 30-dimensional Gaussian, of course we will not show the underlying distribution but only show the samples. We will choose to represent the covariance as\n",
        "$$k(x_i,x_j) = \\exp \\left[-\\frac{(x_i-x_j)^2}{2} \\right]$$"
      ],
      "id": "4ad0b487"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e5d889e"
      },
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "def covkernel(xi,xj,l=1.0):\n",
        "    dij= cdist(xi,xj,metric='euclidean') # returns matrix of pairwise distances\n",
        "    return np.exp(-0.5*dij**2/l**2)\n",
        "\n",
        "ndim     = 30\n",
        "nsamples = 5\n",
        "xrvs     = np.linspace(0,1,ndim)[:,np.newaxis]\n",
        "mycmap = plt.cm.get_cmap('gist_rainbow',nsamples)\n",
        "myColors = [mycmap(i/(nsamples-1.)) for i in range(nsamples)]\n",
        "\n",
        "# define the distribution and draw samples\n",
        "mean=np.zeros(ndim) # zero mean\n",
        "#Xij = covkernel(xrvs,xrvs)\n",
        "cov = covkernel(xrvs,xrvs) # some covariance matrix with correlation present\n",
        "samples = rng.multivariate_normal(mean,cov,nsamples)\n",
        "\n",
        "# Represent the draws by plotting them\n",
        "# as lines connecting the Gaussian vectors\n",
        "fig,axs = plt.subplots(1)\n",
        "for i,sample in enumerate(samples):\n",
        "    axs.plot(xrvs,sample,linestyle='-',marker = 'x',color=myColors[i],linewidth=2)\n",
        "axs.xaxis.set_ticks(list(xrvs))\n",
        "axs.axes.get_xaxis().set_visible(False)\n",
        "#axs.set_xticklabels(['' for i in range(ndim)])\n",
        "#axs.xaxis.set_ticklabels(['' for i in range(ndim)])\n",
        "\n",
        "plt.show()\n"
      ],
      "id": "7e5d889e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ed07164"
      },
      "source": [
        "Now, let's see if we can generate a few \"observations,\" and then sample the posterior. Let's assume our true function is something we know, like the Van der Waals equation of state:\n",
        "\n",
        "$$P_r = \\frac{8}{3}\\frac{T_r}{V_r - \\frac{1}{3}}-\\frac{3}{V_r^2}$$\n",
        "\n",
        "To keep this problem in a physically reasonable range, we will suppose $T_r = 0.85$ and examine $V_r$ over the range of 0.5 to 4. In the following, we can include fewer or more observations to illustrate how our uncertainty changes."
      ],
      "id": "2ed07164"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1c8e112"
      },
      "source": [
        "def vdw_P(Tr,Vr):\n",
        "    term1 = 8.*Tr/(3*Vr-1)\n",
        "    term2 = 3.0/(Vr*Vr)\n",
        "    return term1 - term2\n",
        "\n",
        "def make_plot(Vs,Ps,x,y):\n",
        "    fig,axs = plt.subplots(1)\n",
        "    axs.plot(Vs,Ps,linestyle='-',color='k',linewidth=4)\n",
        "    axs.plot(x[:,0],y[:,0],linestyle='none',marker = 'o',color='r',markersize=10)\n",
        "    axs.set_xlim([0,4])\n",
        "    axs.set_ylim([-1,2])\n",
        "    return fig,axs\n",
        "\n",
        "def make_plot_posterior(axs,fmean,fp,fm):\n",
        "    axs.fill_between(xs[:,0],fm,fp,alpha = 0.2)\n",
        "    axs.plot(xs,fmean,linestyle='-',color='b',linewidth=4)\n",
        "    return\n",
        "\n",
        "# look at a reasonable range of parameters\n",
        "Tr = 0.85\n",
        "Vmin,Vmax = 0.5,4\n",
        "Vs = np.linspace(Vmin,Vmax,1000)\n",
        "Ps = vdw_P(Tr,Vs)\n",
        "\n",
        "# generate random observations (over slightly smaller interval)\n",
        "nobs = 10\n",
        "x    = Vmin + (Vmax-1-Vmin)*rng.random(nobs) # random samples for V\n",
        "x    = x[:,np.newaxis]\n",
        "y    = vdw_P(Tr,x)\n",
        "yavg = np.mean(y)\n",
        "fig,axs = make_plot(Vs,Ps,x,y)\n",
        "\n",
        "# now generate our test points (denoted *)\n",
        "xs = Vs[:,np.newaxis]\n",
        "\n",
        "# we need to get all of the relevant covariant matrices\n",
        "length  = 0.1\n",
        "kxsx    = covkernel(xs,x,l=length)\n",
        "kxx     = covkernel(x,x,l=length)\n",
        "invkxx  = np.linalg.inv(kxx)\n",
        "kxsxs   = covkernel(xs,xs,l=length)\n",
        "kxxs    = covkernel(x,xs,l=length)\n",
        "\n",
        "# now we can sample the posterior with the following\n",
        "# mean and covariance functions. We will generate a number of samples\n",
        "# and then show a few for representation sake\n",
        "# Note: as a \"prior\" to formulate the mean over our functions, we will use an ideal gas\n",
        "# the observe mean\n",
        "muf   = yavg\n",
        "meanf = muf+kxsx@invkxx@(y-yavg) # <-- differs from earlier, when we assumed mean = 0\n",
        "covf  = kxsxs - kxsx@invkxx@kxxs\n",
        "nsamples = 500\n",
        "samples  = rng.multivariate_normal(meanf[:,0],covf,nsamples)\n",
        "sample_mean = np.mean(samples,axis=0)\n",
        "sample_std  = np.std(samples,axis=0)\n",
        "mean_plus   = sample_mean + 1*sample_std\n",
        "mean_minus  = sample_mean - 1*sample_std\n",
        "make_plot_posterior(axs,sample_mean,mean_minus,mean_plus)\n",
        "\n",
        "for i,sample in enumerate(samples[:5]):\n",
        "    axs.plot(xs,sample,linestyle='--',color=myColors[i],linewidth=1)\n",
        "\n",
        "plt.show()"
      ],
      "id": "e1c8e112",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5394dc0f"
      },
      "source": [
        "The above presumes noise-free observations and also uses a naive formulation scheme with direct sampling to obtain our results. The posterior mean and variance can actually be obtained directly as we will show next."
      ],
      "id": "5394dc0f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbde94dd"
      },
      "source": [
        "# new targets with noise:\n",
        "sigma_n = 0.1\n",
        "\n",
        "# add noise to previous observationsgenerate random observations\n",
        "y    = vdw_P(Tr,x) + rng.normal(loc=0,scale=sigma_n,size=nobs)[:,np.newaxis]\n",
        "yavg = np.mean(y)\n",
        "\n",
        "# conventional algorithm\n",
        "length  = 0.1\n",
        "kxsx    = covkernel(xs,x,l=length)\n",
        "kxx     = covkernel(x,x,l=length)\n",
        "kxsxs   = covkernel(xs,xs,l=length)\n",
        "kxxs    = covkernel(x,xs,l=length)\n",
        "\n",
        "#meanf = muf+kxsx@invkxx@(y-yavg)\n",
        "# 1. Cholesky decomposition of K(x,x)\n",
        "L = np.linalg.cholesky(kxx)\n",
        "\n",
        "# 2. Obtain alpha = L.T|(L|y) = L.T/b\n",
        "b     = np.linalg.lstsq(L,y[:,0]-yavg,rcond=None)[0]\n",
        "alpha = np.linalg.lstsq(L.T,b,rcond=None)[0]\n",
        "\n",
        "# 3. Obtain the posterior mean function\n",
        "fmean = yavg + kxxs.T@alpha\n",
        "\n",
        "# 4. obtain posterior variance function\n",
        "v = np.linalg.lstsq(L,kxxs,rcond=None)[0]\n",
        "fvar = np.diag(kxsxs - v.T@v)\n",
        "fstd = np.sqrt(fvar)\n",
        "mean_plus   = fmean + 1*fstd\n",
        "mean_minus  = fmean - 1*fstd\n",
        "\n",
        "#plotting\n",
        "fig,axs = make_plot(Vs,Ps,x,y)\n",
        "make_plot_posterior(axs,fmean,mean_minus,mean_plus)\n",
        "plt.show()"
      ],
      "id": "dbde94dd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccc0ed97"
      },
      "source": [
        "Now, we will simply repeat the exercise but using built-in functions from scikit-learn to do the GPR."
      ],
      "id": "ccc0ed97"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78f74952"
      },
      "source": [
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.gaussian_process   import GaussianProcessRegressor\n",
        "the_kernel = RBF(length_scale=0.1,length_scale_bounds=\"fixed\")\n",
        "the_model  = GaussianProcessRegressor(kernel=the_kernel)\n",
        "the_model.fit(x,y)\n",
        "fmean,fstd = the_model.predict(xs,return_std=True)\n",
        "mean_plus   = fmean[:,0] + 1*fstd\n",
        "mean_minus  = fmean[:,0] - 1*fstd\n",
        "\n",
        "# plotting\n",
        "fig,axs = make_plot(Vs,Ps,x,y)\n",
        "make_plot_posterior(axs,fmean,mean_minus,mean_plus)\n",
        "plt.show()"
      ],
      "id": "78f74952",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07b61f61"
      },
      "source": [
        "Notice that using scikit-learn it presumes that the mean should be zero! This highlights a need that we should probably normalized our data. Let's retry using a little bit of preprocessing..."
      ],
      "id": "07b61f61"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60f7a27c"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "scaler = preprocessing.StandardScaler().fit(y)\n",
        "yscaled= scaler.transform(y)\n",
        "# to keep the comparison similar to before, we will\n",
        "# also account for scaling to unit standard deviation\n",
        "# newer versions of scikit-learn can use the option with_std = False\n",
        "# but mine does not have this, and I do not want to update it right now\n",
        "\n",
        "# fit to transformed y data, then inverse_transform\n",
        "the_kernel = RBF(length_scale=0.1,length_scale_bounds=\"fixed\")\n",
        "the_model  = GaussianProcessRegressor(kernel=the_kernel)\n",
        "the_model.fit(x,yscaled)\n",
        "fmeanscaled,fstdscaled = the_model.predict(xs,return_std=True)\n",
        "fmean       = scaler.inverse_transform(fmeanscaled[:,0].reshape(-1,1)).flatten()\n",
        "mean_plus   = scaler.inverse_transform(fmeanscaled[:,0].reshape(-1,1)+ (1/scaler.scale_)*fstdscaled.reshape(-1,1)).flatten()\n",
        "mean_minus  = scaler.inverse_transform(fmeanscaled[:,0].reshape(-1,1)- (1/scaler.scale_)*fstdscaled.reshape(-1,1)).flatten()\n",
        "\n",
        "#plotting\n",
        "fig,axs = make_plot(Vs,Ps,x,y)\n",
        "\n",
        "make_plot_posterior(axs,fmean,mean_minus,mean_plus)\n",
        "plt.show()"
      ],
      "id": "60f7a27c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY2JR2oCMGlr"
      },
      "source": [
        "Building towards Active Learning."
      ],
      "id": "pY2JR2oCMGlr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03eb3405"
      },
      "source": [
        "# objective function\n",
        "def objective(x, noise=0.1):\n",
        "\tnoise = np.random.normal(loc=0, scale=noise)\n",
        "\treturn (x**2 * np.sin(5 * np.pi * x)**6.0) + noise\n",
        "\n",
        "# grid-based sample of the domain [0,1]\n",
        "X = np.arange(0, 1, 0.01)\n",
        "# sample the domain without noise\n",
        "y = [objective(x, 0) for x in X]\n",
        "# sample the domain with noise\n",
        "ynoise = [objective(x) for x in X]\n",
        "# find best result\n",
        "ix = np.argmax(y)\n",
        "print('Optima: x=%.3f, y=%.3f' % (X[ix], y[ix]))\n",
        "# plot the points with noise\n",
        "plt.scatter(X, ynoise)\n",
        "# plot the points without noise\n",
        "plt.plot(X, y)\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "id": "03eb3405",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4cVChyqMXCw"
      },
      "source": [
        "def surrogate(model, X):\n",
        "\t# catch any warning generated when making a prediction\n",
        "\treturn model.predict(X, return_std=True)\n",
        "\n",
        "# plot real observations vs surrogate function\n",
        "def plot(X, y, model):\n",
        "\t# scatter plot of inputs and real objective function\n",
        "\tplt.scatter(X, y)\n",
        "\t# line plot of surrogate function across domain\n",
        "\tXsamples = np.asarray(np.arange(0, 1, 0.001))\n",
        "\tXsamples = Xsamples.reshape(len(Xsamples), 1)\n",
        "\tysamples, _ = surrogate(model, Xsamples)\n",
        "\tplt.plot(Xsamples, ysamples)\n",
        "\t# show the plot\n",
        "\tplt.show()\n",
        "\n",
        "# sample the domain sparsely with noise\n",
        "X = np.random.random(100)\n",
        "y = np.asarray([objective(x) for x in X])\n",
        "# reshape into rows and cols\n",
        "X = X.reshape(len(X), 1)\n",
        "y = y.reshape(len(y), 1)\n",
        "# define the model\n",
        "model = GaussianProcessRegressor()\n",
        "# fit the model\n",
        "model.fit(X, y)\n",
        "# plot the surrogate function\n",
        "plot(X, y, model)"
      ],
      "id": "w4cVChyqMXCw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EXjIuGqOtCf"
      },
      "source": [
        "# probability of improvement acquisition function\n",
        "def acquisition(X, Xsamples, model):\n",
        "\t# calculate the best surrogate score found so far\n",
        "\tyhat, _ = surrogate(model, X)\n",
        "\tbest = np.max(yhat)\n",
        "\t# calculate mean and stdev via surrogate function\n",
        "\tmu, std = surrogate(model, Xsamples)\n",
        "\tmu = mu[:, 0]\n",
        "\t# calculate the probability of improvement\n",
        "\tprobs = norm.cdf((mu - best) / (std+1E-9))\n",
        "\treturn probs\n",
        "\n",
        "# optimize the acquisition function\n",
        "def opt_acquisition(X, y, model):\n",
        "\t# random search, generate random samples\n",
        "\tXsamples = np.random.random(100)\n",
        "\tXsamples = Xsamples.reshape(len(Xsamples), 1)\n",
        "\t# calculate the acquisition function for each sample\n",
        "\tscores = acquisition(X, Xsamples, model)\n",
        "\t# locate the index of the largest scores\n",
        "\tix = np.argmax(scores)\n",
        "\treturn Xsamples[ix, 0]\n",
        "\n",
        "\n",
        "# sample the domain sparsely with noise\n",
        "X = np.random.random(100)\n",
        "y = np.asarray([objective(x) for x in X])\n",
        "# reshape into rows and cols\n",
        "X = X.reshape(len(X), 1)\n",
        "y = y.reshape(len(y), 1)\n",
        "# define the model\n",
        "model = GaussianProcessRegressor()\n",
        "# fit the model\n",
        "model.fit(X, y)\n",
        "# plot before hand\n",
        "plot(X, y, model)\n",
        "# perform the optimization process\n",
        "for i in range(100):\n",
        "\t# select the next point to sample\n",
        "\tx = opt_acquisition(X, y, model)\n",
        "\t# sample the point\n",
        "\tactual = objective(x)\n",
        "\t# summarize the finding\n",
        "\test, _ = surrogate(model, [[x]])\n",
        "\tprint('>x=%.3f, f()=%3f, actual=%.3f' % (x, est, actual))\n",
        "\t# add the data to the dataset\n",
        "\tX = np.vstack((X, [[x]]))\n",
        "\ty = np.vstack((y, [[actual]]))\n",
        "\t# update the model\n",
        "\tmodel.fit(X, y)\n",
        "\n",
        "# plot all samples and the final surrogate function\n",
        "plot(X, y, model)\n",
        "# best result\n",
        "ix = np.argmax(y)\n",
        "print('Best Result: x=%.3f, y=%.3f' % (X[ix], y[ix]))"
      ],
      "id": "0EXjIuGqOtCf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-6lHJ2MVf0h"
      },
      "source": [
        "Now, let's simply explore the space. Pick the next point to sample using the predicted uncertainty."
      ],
      "id": "t-6lHJ2MVf0h"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2hG6pfrREm-"
      },
      "source": [
        "### BEGIN SOLUTION\n",
        "\n",
        "\n",
        "### END SOLUTION"
      ],
      "id": "G2hG6pfrREm-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsn6c3AVXA73"
      },
      "source": [],
      "id": "dsn6c3AVXA73",
      "execution_count": null,
      "outputs": []
    }
  ]
}