{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA93DwTFTRvY"
      },
      "source": [
        "#Molecular Featurization and Descriptors\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJoWTlPATY9Y"
      },
      "source": [
        "!pip install rdkit-pypi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Dbqv0KrRXl4"
      },
      "source": [
        "import numpy  as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow       import keras\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler,MaxAbsScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression,Ridge,Lasso\n",
        "import sklearn.metrics as sklm\n",
        "import pydot\n",
        "import graphviz\n",
        "\n",
        "# Here are some new things\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import PandasTools as PT\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS__EGAsVh3p"
      },
      "source": [
        "Figuring out our vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-ZrX39ZTuUJ"
      },
      "source": [
        "mols = ['N#N','O=O','[HH]','[He]','C','C(=O)=O']\n",
        "# Let's determine our vocabulary over the set provided\n",
        "# this code does not work for parsing SMILES, in general\n",
        "# we just want to keep things simples\n",
        "vocab = set()\n",
        "\n",
        "# iterate over all molecules to extract unique characters\n",
        "M = 0 # to contain maximum number of characters in any string...\n",
        "for mol in mols:\n",
        "    mol = list(mol)\n",
        "    if len(mol) > M:\n",
        "        M = len(mol)\n",
        "    while mol:\n",
        "        v = mol.pop()\n",
        "        if v not in vocab:\n",
        "            vocab.add(v)\n",
        "print(vocab)\n",
        "print(len(vocab))\n",
        "# Now we will create a mapping dictionary that\n",
        "# points each character to a one-hot vector\n",
        "N  = len(vocab)\n",
        "IN = np.eye(N)\n",
        "charMap = {}\n",
        "for i,c in enumerate(sorted(list(vocab))):\n",
        "    charMap[c] = IN[i,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQdqIKkRVy63"
      },
      "source": [
        "Take SMILES and turn it into an array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIIWgN9yVkOz"
      },
      "source": [
        "''' function that takes a SMILES string\n",
        "and returns a (padded) array with ohe\n",
        "vectors as indicated by CMap\n",
        "M - dimension to pad to if necessary\n",
        "N - dimension of OHE '''\n",
        "def Smi2Arr(SmiStr,CMap,M,N):\n",
        "    Arr   = np.zeros([M,N])\n",
        "    for i,c in enumerate(list(SmiStr)):\n",
        "        Arr[i,:] = CMap[c][:]\n",
        "    return Arr\n",
        "\n",
        "# create feature arrays\n",
        "featArr = []\n",
        "for mol in mols:\n",
        "    featArr.append(Smi2Arr(mol,charMap,M,N))\n",
        "\n",
        "print(featArr)\n",
        "print(len(featArr))\n",
        "print(len(featArr[0]))\n",
        "print(len(featArr[0][0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLfThObyV-dw"
      },
      "source": [
        "Visualize the fingerprints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlPXqq97V2yp"
      },
      "source": [
        "# visualize the encodings\n",
        "\n",
        "def make_plot(ax,mol,Arr):\n",
        "    ax.imshow(Arr.T)\n",
        "    ax.axes.xaxis.set_visible(False)\n",
        "    ax.axes.yaxis.set_visible(False)\n",
        "    ax.set_title(mol)\n",
        "    plt.grid(True,which='minor',color='w')\n",
        "\n",
        "fig, axs = plt.subplots(1,len(mols))\n",
        "\n",
        "for i, (mol,Arr) in enumerate(zip(mols,featArr)):\n",
        "    make_plot(axs[i],mol,Arr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3942224b"
      },
      "source": [
        "# Importing some data\n",
        "The first we will do is just get our hands on some chemical structures to muck around with. We will use a dataset derived from the \"Toxicology in the 21st Century\" initiative. This contains qualitative toxicitity measurements for ~8000 compounds for 12 different targets (e.g., nuclear receptors and stress response pathways for different stuff). Use the following cell to load and preview the data. All of the columns beginning `NR-` or `SR-` are potential labels, while we plan to convert the `smiles` column into features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acbd97e0"
      },
      "source": [
        "data        = pd.read_csv(\"tox21.csv\")\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d47cf29c"
      },
      "source": [
        "Now, we are going to extract just the SMILES strings and demonstrate a nice little utility to create molecular structures. Is that necessary? No. Is it something we can do? Watch and find out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf39397c"
      },
      "source": [
        "label  = 'NR-AR'\n",
        "exdata = data[['smiles',label]]\n",
        "PT.AddMoleculeColumnToFrame(exdata,smilesCol='smiles',includeFingerprints=False)\n",
        "exdata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7af7f5a6"
      },
      "source": [
        "Notice that some of the data above contains NaN -- probably because there is not data for that label for that molecule. We can eliminate these entries from the dataframe. Then we will make a little diagram to illustrate the toxicity values along with the molecular structures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "384b5cad"
      },
      "source": [
        "exdata= exdata[~exdata['ROMol'].isnull()]\n",
        "exdata= exdata[~exdata[label].isnull()]\n",
        "exdata.head()\n",
        "display(PT.FrameToGridImage(exdata[exdata[label]>0.95].head(5), legendsCol=label, molsPerRow=5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55f66906"
      },
      "source": [
        "## RDKit Molecules\n",
        "Now it's time to stop fooling around and get down to business. The first thing we are going to do is introduce how to convert SMILES into molecule objects in RDKit. Once we have a molecule object, RDKit can be used for many things. Is there any good way to know everything that can be done within RDKit? Not really... A lot of jumping around on doc pages, tutorials, and trial-and-error.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1c4e328"
      },
      "source": [
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "from rdkit.Chem import Draw\n",
        "exampleStr = exdata['smiles'][0]\n",
        "exampleMol = Chem.MolFromSmiles(exampleStr)\n",
        "print(exampleStr)\n",
        "\n",
        "# Here I will show how to check what types of methods exist\n",
        "# within the molecule object.\n",
        "mol_methods = [method_name for method_name in dir(exampleMol)]\n",
        "for mol_method in mol_methods:\n",
        "    print(mol_method)\n",
        "exampleMol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "883a27a9"
      },
      "source": [
        "Molecules are made of atoms (duh) and these also have properties"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "812c6e86"
      },
      "source": [
        "ringStuff = exampleMol.GetRingInfo()\n",
        "for atom in exampleMol.GetAtoms():\n",
        "    num = atom.GetAtomicNum()\n",
        "    idx = atom.GetIdx()\n",
        "    ri  = ringStuff.NumAtomRings(idx)\n",
        "    print(\"Atom {} with atomic number {} is in {} rings\".format(idx,num,ri))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20cf8659"
      },
      "source": [
        "## Fingerprints\n",
        "OK, so RDKit can be pretty useful for all kinds of random things (e.g., creating automated images of molecules, identifying substructures, and so on.) As it goes for machine learning, two areas of emphasis will be in the acquisition of fingerprints and molecular descriptors. RDKit offers a few different kind of fingerprints. The `RDKit fingerprint` is something akin to the `Daylight fingerprint` -- but it is not that. Meanwhile, you can also use `Morgan fingerprints`. Below we will demonstrate the generation of these fingerprints while adding a second example molecule so we can compute similarities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3971cea2"
      },
      "source": [
        "exampleStr = exdata['smiles'][6]\n",
        "exampleMol2 = Chem.MolFromSmiles(exampleStr)\n",
        "print(exampleStr)\n",
        "exampleMol2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2601e064"
      },
      "source": [
        "# Now we will create fingerprints.\n",
        "mols = [exampleMol, exampleMol2] # just a list of molecule objects\n",
        "\n",
        "# Create fingerprint objects\n",
        "# fingerprint objects contain a set of operations; because fps may be sparse\n",
        "# they are packaged in classes that may not be directly usable for ML applications\n",
        "# we can convert them to numpy arrays though if that is our purpose\n",
        "\n",
        "#These are the fingerprints\n",
        "RDfps= [Chem.RDKFingerprint(m) for m in mols] # list of RDKit fingerprints\n",
        "Mfps = [Chem.rdMolDescriptors.GetMorganFingerprintAsBitVect(m,4) for m in mols]\n",
        "\n",
        "# these are output to numpy arrays\n",
        "RDvecs = [np.array(fp) for fp in RDfps]\n",
        "Mvecs  = [np.array(fp) for fp in Mfps ]\n",
        "\n",
        "# We can see that these fingerprints are very different\n",
        "fig,ax = plt.subplots(2,1)\n",
        "fig.set_size_inches(64,16)\n",
        "ax[0].plot(RDvecs[0],'ok')\n",
        "ax[1].plot(Mvecs[0],'or')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c7af257"
      },
      "source": [
        "## Similarity Calculations\n",
        "One thing that can be useful is to express measures of similarity for molecules. There are numerous metrics that are floating around for doing things of this kind. As far as their documentation reports, RDKit enables facile calculation of the following similarity metrics: `Tanimoto, Dice, Cosine, Sokal, Russel, Kulczynski, McConnaughey, and Tversky`\n",
        "Inspecting the methods under `DataStructs` seems to suggest that they have even more beyond this.\n",
        "\n",
        "In the following we will make a comparison of similarities for some different metrics, as well as the different fingerprints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1121a52b"
      },
      "source": [
        "from rdkit import DataStructs\n",
        "metType   = ['Tanimoto', 'Dice', 'Cosine', 'Sokal', 'Russel', 'Kulczynski','McConnaughey']\n",
        "simScores = np.zeros((2,len(metType)))\n",
        "\n",
        "# ordinarily you would invoke a call to the method like\n",
        "# DataStructs.FingerprintSimilarity(Mfps[0],Mfps[1],   metric=DataStructs.DiceSimilarity)\n",
        "# to iterate over the strings contained within metType, I will recognize that this things are classes with attributes\n",
        "for i,metric in enumerate(metType):\n",
        "    metricName = metric + \"Similarity\"\n",
        "    simScores[0,i] = DataStructs.FingerprintSimilarity(RDfps[0],RDfps[1], metric=getattr(DataStructs,metricName))\n",
        "    simScores[1,i] = DataStructs.FingerprintSimilarity(Mfps[0],Mfps[1], metric=getattr(DataStructs,metricName))\n",
        "\n",
        "plt.plot(simScores[0,:],'-ok')\n",
        "plt.plot(simScores[1,:],'-or')\n",
        "# https://jcheminf.biomedcentral.com/articles/10.1186/s13321-015-0069-3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e58ad97"
      },
      "source": [
        "## Descriptors in RDKit and Mordred\n",
        "From molecule objects, we can also compute descriptors using RDKit. There are probably a zillion ways to go about doing this but there is a reasonable module `rdkit.Chem.rdMolDescriptors` that has a list of different things that can be computed. These can range from simple functions of connectivity/chemistry to geomteric descriptions and approximate surface area calculations; some don't even have documentation but still exists. We will just demonstrate a couple of these."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f595cb26"
      },
      "source": [
        "# Below we compute the \"Labute Accessible Surface Area\", Number of Rings, and Number of Rotatable bonds\n",
        "LASA = [0.0 for i in range(exdata.shape[0])]\n",
        "NR   = [0.0 for i in range(exdata.shape[0])]\n",
        "NRB  = [0.0 for i in range(exdata.shape[0])]\n",
        "for i,smiles in enumerate(exdata['smiles']):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    LASA[i] = np.round(Chem.rdMolDescriptors.CalcLabuteASA(mol),2)\n",
        "    NR[i]   = Chem.rdMolDescriptors.CalcNumRings(mol)\n",
        "    NRB[i]  = Chem.rdMolDescriptors.CalcNumRotatableBonds(mol)\n",
        "\n",
        "exdata['LASA'] = LASA\n",
        "exdata['NR']  = NR\n",
        "exdata['NRB'] = NRB\n",
        "pd.options.display.float_format = \"{:,.2f}\".format\n",
        "print(\"Examples of LASA\")\n",
        "display(PT.FrameToGridImage(exdata.head(15), legendsCol='LASA', molsPerRow=5))\n",
        "print(\"\\n Examples of Number of Rings\")\n",
        "display(PT.FrameToGridImage(exdata.tail(15), legendsCol='NR', molsPerRow=5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63de418c"
      },
      "source": [
        "So whilst RDKit supplies a lot of functionality in terms of computing these various descriptors, but honestly, it is a bit cumbersome. That's where Mordred comes in. In the following, we will import the mordred calculator and some list of descriptors and apply that to the data in the pandas dataframe. Mordred computes all the descriptors available in RDKit (or claims to) as well as some others. Running the below cell may take a few minutes, since there are *a lot* (1500+) descriptors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5563b2b6"
      },
      "source": [
        "!pip install mordred\n",
        "from mordred import Calculator, descriptors\n",
        "calculator = Calculator(descriptors,ignore_3D=True)\n",
        "wdesc = calculator.pandas(exdata['ROMol'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "075aee36"
      },
      "source": [
        "# the below will prune away any descriptors that did not have valid calculations\n",
        "from mordred.error import Missing\n",
        "missing = []\n",
        "for column in wdesc.columns:\n",
        "    if (wdesc[column].apply(lambda x: type(x) == Missing)).any():\n",
        "        missing.append(column)\n",
        "wdesc = wdesc.drop(missing,axis=1)\n",
        "wdesc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1540f1ab"
      },
      "source": [
        "# Predicting Class from Descriptors using Random Forest\n",
        "Below we will demonstrate the construction of a random forest classifier that decides the class of whatever toxicity value we picked based on the set of descriptors. In addition to showing the confusion matrix, we also indicate the area under a ROC curve as another metric of model accuracy. We did not discuss ROC in class, but a perfect classifier has an area of 1.0 under the ROC curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a395b672"
      },
      "source": [
        "labels  = np.array(exdata[label]).reshape([-1,1])\n",
        "features= np.array(wdesc)\n",
        "X_train,X_test,y_train,y_test = train_test_split(features,labels,test_size = 0.2)\n",
        "RFC = RandomForestClassifier(n_estimators=200)\n",
        "RFC.fit(X_train,y_train)\n",
        "y_pred = np.squeeze(RFC.predict(X_test))\n",
        "p_pred = RFC.predict_proba(X_test)[:,1]\n",
        "C = sklm.confusion_matrix(np.squeeze(y_test),y_pred)\n",
        "roc = sklm.roc_auc_score(np.squeeze(y_test),p_pred)\n",
        "print(C)\n",
        "print(roc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f5b6702"
      },
      "source": [
        "## Predicting Class from Descriptors using a Neural Network\n",
        "We will see if we can achieve similar results by doing classification with a neural network. As a note, it is possible to build neural networks in scikit learn... In fact, they are probably even easier in Keras, and you should feel capabile of doing it. That being said, we'll do this with Keras, which is more powerful on the whole."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1f67642"
      },
      "source": [
        "# Create Model Container\n",
        "NNC = keras.Sequential(name=\"NNClassifier\")\n",
        "\n",
        "# Define Layers\n",
        "inputLayer= keras.Input(shape=(wdesc.shape[1],))\n",
        "hidden_neurons = [1000,500,250,100,50,20]\n",
        "NNC.add(inputLayer)\n",
        "for n in hidden_neurons:\n",
        "    NNC.add(layers.Dense(n,activation='relu'))\n",
        "output= layers.Dense(1,activation='sigmoid')\n",
        "NNC.add(output)\n",
        "\n",
        "# Admire Model\n",
        "NNC.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4122562"
      },
      "source": [
        "# Define Model optimization\n",
        "X_train = np.asarray(X_train).astype('float32')\n",
        "y_train = np.asarray(y_train).astype('float32')\n",
        "\n",
        "myScaler  = MaxAbsScaler().fit(features)\n",
        "Xsc_train = myScaler.transform(X_train)\n",
        "Xsc_test  = myScaler.transform(X_test)\n",
        "my_optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "NNC.compile(optimizer=my_optimizer,loss=\"binary_crossentropy\")\n",
        "my_callbacks = [tf.keras.callbacks.EarlyStopping(patience=50)]\n",
        "hist = NNC.fit(x=Xsc_train,y=y_train,epochs=500,batch_size=256,callbacks=my_callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b1ed4c5"
      },
      "source": [
        "p_pred = NNC.predict(np.asarray(Xsc_test).astype('float32'))\n",
        "y_pred = np.round(p_pred,0)\n",
        "C = sklm.confusion_matrix(np.squeeze(y_test),y_pred)\n",
        "roc = sklm.roc_auc_score(np.squeeze(y_test),p_pred)\n",
        "print(C)\n",
        "print(roc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a367578"
      },
      "source": [
        "The performance that we got for the NN classifier in Keras is actually a bit worse than that of random forest. That is not necessarily surprising, but we should also keep in mind that we have spent no time trying to optimize the architecture of the model. Interestingly, the next cell, I try the same problem in scikit-learn, and I end up with somewhat better results. This seems to suggest that my choice of hyperparameters could be better, or scikit-learn is doing sometinbg else under the hood that has led to somewhat better results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c55b60ff"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "clf = MLPClassifier(hidden_layer_sizes=[1000,500,250,100,50,20]).fit(Xsc_train, y_train)\n",
        "(roc_auc_score(y_test, clf.predict_proba(Xsc_test)[:,1]), clf.score(Xsc_train,y_train),clf.score(Xsc_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3262b652"
      },
      "source": [
        "## Using Fingerprints as Features\n",
        "In the previous few cells, our features were defined by a vector of descriptors, many of which are derived from the chemical structure. It would seem possible that we could train machine learning models to learn directly from a description of the chemical structure itself. Let's try it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9cb49f1"
      },
      "source": [
        "fps = []\n",
        "for i,smiles in enumerate(exdata['smiles']):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    fps.append(Chem.rdMolDescriptors.GetMorganFingerprintAsBitVect(mol,5))\n",
        "    #fps.append(Chem.RDKFingerprint(mol))\n",
        "\n",
        "features = np.array(fps)\n",
        "print(features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "537eab5a"
      },
      "source": [
        "As you can see above, the default size of the fingerprint is 2048-dimensions... that would necessitate a whole lot of parameters for our machine learning model. Let's take a look at one of these fingerprints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "834412cd"
      },
      "source": [
        "print(list(fps[0]))\n",
        "print(list(fps[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64c34615"
      },
      "source": [
        "Hmmm, a lot of ones and zeros that indicate the presence or lack of some chemical substructure (roughly speaking)... It can sometimes be the case that *none* of the molecules in our dataset feature some substructures that might be described in this 2048-dimensional substructure, suggesting that this element is zero across our dataset. Conversely, perhaps it is possible that an element is always one for *all* of the molecules in the dataset. From an ML perspective, these input elements would never differ for any feature vector and thus represent a constant to the model. So we can screen for these to potentially reduce the dimensionality of the feature vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f4b1665"
      },
      "source": [
        "# to figure out if there are any constant elements we will sum all of our fingerprints\n",
        "# any bit that remains zero means that it is zero for everything\n",
        "# any bit that equals the number of data points means it is one for everything\n",
        "featureSum = np.sum(features,axis=0)\n",
        "alwaysOn   = featureSum > features.shape[0]-0.01\n",
        "alwaysOff  = featureSum < 0.5\n",
        "print(\"There are {:>4d} elements that are always on\".format(np.sum(alwaysOn)))\n",
        "print(\"There are {:>4d} elements that are always off\".format(np.sum(alwaysOff)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ef114f1"
      },
      "source": [
        "Seems like this isn't the case here, indicating a pretty diverse set of structures. Now let's try out some learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "217e09a3"
      },
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(features,labels,test_size = 0.2)\n",
        "clf = MLPClassifier(hidden_layer_sizes=[1000,500,250]).fit(X_train, y_train)\n",
        "y_pred = np.round(clf.predict_proba(X_test)[:,1],0)\n",
        "C = sklm.confusion_matrix(np.squeeze(y_test),y_pred)\n",
        "print((roc_auc_score(y_test, clf.predict_proba(X_test)[:,1]), clf.score(X_train,y_train),clf.score(X_test,y_test)))\n",
        "print(C)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSl6PrOyxkPL"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}